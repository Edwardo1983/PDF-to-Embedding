{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF-to-Embedding Pipeline\n",
    "## Procesare 15GB Manuale È˜colare â†’ Supabase pgvector\n",
    "\n",
    "Notebook principal pentru Kaggle P100 GPU processing\n",
    "\n",
    "**Setup:**\n",
    "1. Accelerator: GPU (P100)\n",
    "2. Internet: Enable\n",
    "3. Secrets: SUPABASE_URL, SUPABASE_ANON_KEY\n",
    "4. Attached dataset: materiale-didactice folder\n",
    "\n",
    "**Estimated runtime:** 18-24 ore pentru 15GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup initial - verify GPU È™i install dependencies\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify GPU\n",
    "print(\"Checking GPU availability...\")\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "!pip install -q PyMuPDF>=1.23.0 paddleocr>=2.7.0 sentence-transformers>=2.2.2 supabase>=2.0.0 tqdm pyyaml\n",
    "\n",
    "print(\"\\nâœ… Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Import Modules & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import yaml\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ObÈ›ine Supabase secrets\n",
    "logger.info(\"Loading Supabase credentials from secrets...\")\n",
    "secret = UserSecretsClient()\n",
    "\n",
    "try:\n",
    "    SUPABASE_URL = secret.get_secret('SUPABASE_URL')\n",
    "    SUPABASE_KEY = secret.get_secret('SUPABASE_ANON_KEY')\n",
    "    logger.info(f\"âœ… Loaded credentials for: {SUPABASE_URL[:30]}...\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"âŒ Failed to load secrets: {e}\")\n",
    "    logger.error(\"Make sure to add SUPABASE_URL and SUPABASE_ANON_KEY in notebook Settings â†’ Secrets\")\n",
    "    raise\n",
    "\n",
    "# Load config\n",
    "logger.info(\"Loading configuration...\")\n",
    "with open('/kaggle/input/') as f:  # Will be attached dataset\n",
    "    try:\n",
    "        # Try to load from config if available\n",
    "        config_path = '/kaggle/working/config.yaml'  # Local copy\n",
    "    except:\n",
    "        # Use defaults\n",
    "        pass\n",
    "\n",
    "config = {\n",
    "    'pdf': {'chunk_size': 500, 'overlap': 50, 'min_chunk_length': 50},\n",
    "    'ocr': {'enabled': True, 'languages': ['ro', 'en'], 'min_image_size': 51200, 'priority_threshold': 0.8},\n",
    "    'embeddings': {'model': 'paraphrase-multilingual-mpnet-base-v2', 'batch_size': 128, 'device': 'cuda'},\n",
    "    'supabase': {'batch_size': 10000, 'max_retries': 3}\n",
    "}\n",
    "\n",
    "logger.info(f\"âœ… Configuration loaded\")\n",
    "logger.info(f\"  Chunk size: {config['pdf']['chunk_size']} chars\")\n",
    "logger.info(f\"  Embedding model: {config['embeddings']['model']}\")\n",
    "logger.info(f\"  Device: {config['embeddings']['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import yaml\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ObÈ›ine Supabase secrets\n",
    "logger.info(\"Loading Supabase credentials from secrets...\")\n",
    "secret = UserSecretsClient()\n",
    "\n",
    "try:\n",
    "    SUPABASE_URL = secret.get_secret('SUPABASE_URL')\n",
    "    SUPABASE_KEY = secret.get_secret('SUPABASE_ANON_KEY')\n",
    "    logger.info(f\"âœ… Loaded credentials for: {SUPABASE_URL[:30]}...\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"âŒ Failed to load secrets: {e}\")\n",
    "    logger.error(\"Make sure to add SUPABASE_URL and SUPABASE_ANON_KEY in notebook Settings â†’ Secrets\")\n",
    "    raise\n",
    "\n",
    "# Load config\n",
    "logger.info(\"Loading configuration...\")\n",
    "with open('/kaggle/input/') as f:  # Will be attached dataset\n",
    "    try:\n",
    "        # Try to load from config if available\n",
    "        config_path = '/kaggle/working/config.yaml'  # Local copy\n",
    "    except:\n",
    "        # Use defaults\n",
    "        pass\n",
    "\n",
    "config = {\n",
    "    'pdf': {'chunk_size': 500, 'overlap': 50, 'min_chunk_length': 50},\n",
    "    'ocr': {'enabled': True, 'languages': ['ro', 'en'], 'min_image_size': 51200, 'priority_threshold': 0.8},\n",
    "    'embeddings': {'model': 'paraphrase-multilingual-mpnet-base-v2', 'batch_size': 128, 'device': 'cuda'},\n",
    "    'supabase': {'batch_size': 10000, 'max_retries': 3}\n",
    "}\n",
    "\n",
    "logger.info(f\"âœ… Configuration loaded\")\n",
    "logger.info(f\"  Chunk size: {config['pdf']['chunk_size']} chars\")\n",
    "logger.info(f\"  Embedding model: {config['embeddings']['model']}\")\n",
    "logger.info(f\"  Device: {config['embeddings']['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Find & List PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all PDFs in mounted dataset\n",
    "logger.info(\"\\nSearching for PDFs...\")\n",
    "\n",
    "# Locate dataset folder\n",
    "kaggle_input = Path('/kaggle/input')\n",
    "dataset_folders = [d for d in kaggle_input.iterdir() if d.is_dir()]\n",
    "\n",
    "if not dataset_folders:\n",
    "    logger.error(\"âŒ No dataset folder found in /kaggle/input/\")\n",
    "    logger.error(\"Make sure to attach dataset before running!\")\n",
    "    raise FileNotFoundError(\"Dataset not found\")\n",
    "\n",
    "# Use first dataset folder\n",
    "pdf_folder = dataset_folders[0]\n",
    "logger.info(f\"Using dataset: {pdf_folder.name}\")\n",
    "\n",
    "# Find all PDFs recursively\n",
    "all_pdfs = list(pdf_folder.glob('**/*.pdf'))\n",
    "\n",
    "logger.info(f\"\\nâœ… Found {len(all_pdfs)} PDFs\")\n",
    "logger.info(f\"Sample PDFs:\")\n",
    "for pdf in all_pdfs[:5]:\n",
    "    size_mb = pdf.stat().st_size / 1024 / 1024\n",
    "    logger.info(f\"  - {pdf.relative_to(pdf_folder)} ({size_mb:.1f} MB)\")\n",
    "\n",
    "if len(all_pdfs) > 5:\n",
    "    logger.info(f\"  ... and {len(all_pdfs) - 5} more\")\n",
    "\n",
    "# Calculate total size\n",
    "total_size_gb = sum(p.stat().st_size for p in all_pdfs) / 1024 / 1024 / 1024\n",
    "logger.info(f\"\\nTotal dataset size: {total_size_gb:.1f} GB\")\n",
    "logger.info(f\"Processing time estimate: 18-25 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Define Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import processing modules directly into notebook\n",
    "# (Alternative: download from GitHub or copy from scripts folder)\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from supabase import create_client\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "logger.info(\"Processing functions initialized\")\n",
    "\n",
    "# ===== PDF EXTRACTION =====\n",
    "def extract_text_simple(pdf_path: str) -> dict:\n",
    "    \"\"\"Simplu PDF text extraction cu PyMuPDF\"\"\"\n",
    "    try:\n",
    "        pdf = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in pdf:\n",
    "            text += page.get_text()\n",
    "        pdf.close()\n",
    "        return {'text': text, 'pages': len(pdf), 'status': 'success'}\n",
    "    except Exception as e:\n",
    "        return {'text': '', 'pages': 0, 'status': 'error', 'error': str(e)}\n",
    "\n",
    "# ===== TEXT CHUNKING =====\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> list:\n",
    "    \"\"\"SplitteazÄƒ text Ã®n chunks cu overlap\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    \n",
    "    for sent in sentences:\n",
    "        if len(current) + len(sent) > chunk_size:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "            current = sent\n",
    "        else:\n",
    "            current += \" \" + sent if current else sent\n",
    "    \n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# ===== DEDUPLICATION =====\n",
    "def deduplicate_chunks(chunks: list) -> list:\n",
    "    \"\"\"Elimina duplicate chunks (headers/footers)\"\"\"\n",
    "    seen_hashes = set()\n",
    "    unique = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_hash = hashlib.md5(chunk.encode()).hexdigest()\n",
    "        if chunk_hash not in seen_hashes and len(chunk) >= 50:\n",
    "            seen_hashes.add(chunk_hash)\n",
    "            unique.append(chunk)\n",
    "    \n",
    "    return unique\n",
    "\n",
    "# ===== EMBEDDING GENERATION =====\n",
    "def init_embedding_model(device='cuda'):\n",
    "    \"\"\"Initialize sentence-transformers model\"\"\"\n",
    "    logger.info(f\"Loading embedding model on {device}...\")\n",
    "    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device=device)\n",
    "    return model\n",
    "\n",
    "def generate_embeddings(texts: list, model, batch_size: int = 128) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for texts\"\"\"\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "# ===== SUPABASE UPLOAD =====\n",
    "def init_supabase(url: str, key: str):\n",
    "    \"\"\"Initialize Supabase client\"\"\"\n",
    "    return create_client(url, key)\n",
    "\n",
    "def upload_vectors_batch(supabase, vectors: list, batch_size: int = 10000) -> dict:\n",
    "    \"\"\"Upload vectors to Supabase in batches\"\"\"\n",
    "    success = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        batch = vectors[i:i+batch_size]\n",
    "        try:\n",
    "            response = supabase.table('document_embeddings').insert(batch).execute()\n",
    "            success += len(batch)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Batch {i//batch_size} failed: {e}\")\n",
    "            failed += len(batch)\n",
    "    \n",
    "    return {'success': success, 'failed': failed}\n",
    "\n",
    "logger.info(\"âœ… All processing functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Main Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing pipeline\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"STARTING MAIN PROCESSING PIPELINE\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "# Initialize components\n",
    "logger.info(\"\\nInitializing components...\")\n",
    "embedding_model = init_embedding_model(device=config['embeddings']['device'])\n",
    "supabase = init_supabase(SUPABASE_URL, SUPABASE_KEY)\n",
    "logger.info(\"âœ… Components initialized\")\n",
    "\n",
    "# Statistics\n",
    "stats = {\n",
    "    'processed_pdfs': 0,\n",
    "    'failed_pdfs': 0,\n",
    "    'total_text_chars': 0,\n",
    "    'total_chunks': 0,\n",
    "    'total_embeddings': 0,\n",
    "    'uploaded_vectors': 0,\n",
    "    'failed_uploads': 0,\n",
    "    'start_time': time.time()\n",
    "}\n",
    "\n",
    "# Process each PDF\n",
    "logger.info(f\"\\nProcessing {len(all_pdfs)} PDFs...\")\n",
    "\n",
    "# Optional: limit for testing\n",
    "TEST_MODE = False  # Set to True to process only 10 PDFs\n",
    "pdfs_to_process = all_pdfs[:10] if TEST_MODE else all_pdfs\n",
    "\n",
    "with tqdm(total=len(pdfs_to_process), desc=\"PDF Processing\") as pbar:\n",
    "    current_batch = []\n",
    "    batch_size = config['supabase']['batch_size']\n",
    "    \n",
    "    for pdf_path in pdfs_to_process:\n",
    "        try:\n",
    "            # 1. Extract text\n",
    "            result = extract_text_simple(str(pdf_path))\n",
    "            \n",
    "            if result['status'] != 'success':\n",
    "                stats['failed_pdfs'] += 1\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            text = result['text']\n",
    "            stats['total_text_chars'] += len(text)\n",
    "            stats['processed_pdfs'] += 1\n",
    "            \n",
    "            # 2. Chunk text\n",
    "            chunks = chunk_text(text, config['pdf']['chunk_size'], config['pdf']['overlap'])\n",
    "            chunks = deduplicate_chunks(chunks)\n",
    "            stats['total_chunks'] += len(chunks)\n",
    "            \n",
    "            # 3. Generate embeddings\n",
    "            embeddings = generate_embeddings(chunks, embedding_model, config['embeddings']['batch_size'])\n",
    "            stats['total_embeddings'] += len(embeddings)\n",
    "            \n",
    "            # 4. Prepare for upload\n",
    "            pdf_name = pdf_path.relative_to(pdf_folder)\n",
    "            \n",
    "            for j, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
    "                vector_item = {\n",
    "                    'chunk_id': f\"{pdf_path.stem}_{j}\",\n",
    "                    'text': chunk[:5000],\n",
    "                    'embedding': emb.tolist(),\n",
    "                    'source_pdf': str(pdf_name),\n",
    "                    'page_num': 1,\n",
    "                    'clasa': 1,\n",
    "                    'materie': 'Various',\n",
    "                    'capitol': 'Chapter',\n",
    "                    'chunk_hash': hashlib.md5(chunk.encode()).hexdigest(),\n",
    "                    'has_images': False\n",
    "                }\n",
    "                current_batch.append(vector_item)\n",
    "            \n",
    "            # 5. Upload batch if full\n",
    "            if len(current_batch) >= batch_size:\n",
    "                result = upload_vectors_batch(supabase, current_batch, batch_size)\n",
    "                stats['uploaded_vectors'] += result['success']\n",
    "                stats['failed_uploads'] += result['failed']\n",
    "                current_batch = []\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing {pdf_path.name}: {e}\")\n",
    "            stats['failed_pdfs'] += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Upload remaining batch\n",
    "    if current_batch:\n",
    "        result = upload_vectors_batch(supabase, current_batch, batch_size)\n",
    "        stats['uploaded_vectors'] += result['success']\n",
    "        stats['failed_uploads'] += result['failed']\n",
    "\n",
    "logger.info(\"âœ… Main processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Print Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final statistics\n",
    "elapsed = time.time() - stats['start_time']\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"PROCESSING COMPLETE - FINAL STATISTICS\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "logger.info(f\"\\nProcessing time: {elapsed/3600:.1f} hours\")\n",
    "logger.info(f\"\\nPDF Processing:\")\n",
    "logger.info(f\"  âœ“ Successful: {stats['processed_pdfs']}\")\n",
    "logger.info(f\"  âœ— Failed: {stats['failed_pdfs']}\")\n",
    "logger.info(f\"\\nText Statistics:\")\n",
    "logger.info(f\"  Total text: {stats['total_text_chars']:,} characters\")\n",
    "logger.info(f\"  Average per PDF: {stats['total_text_chars']//max(stats['processed_pdfs'],1):,} chars\")\n",
    "logger.info(f\"\\nChunking:\")\n",
    "logger.info(f\"  Total chunks: {stats['total_chunks']:,}\")\n",
    "logger.info(f\"  Chunks per PDF: {stats['total_chunks']//max(stats['processed_pdfs'],1)}\")\n",
    "logger.info(f\"\\nEmbeddings:\")\n",
    "logger.info(f\"  Total vectors: {stats['total_embeddings']:,}\")\n",
    "logger.info(f\"  Vector dimension: 768\")\n",
    "logger.info(f\"\\nSupabase Upload:\")\n",
    "logger.info(f\"  âœ“ Uploaded: {stats['uploaded_vectors']:,}\")\n",
    "logger.info(f\"  âœ— Failed: {stats['failed_uploads']}\")\n",
    "\n",
    "logger.info(f\"\\n\" + \"=\"*70)\n",
    "logger.info(f\"Estimated database size: {(stats['total_embeddings'] * 3.5 / 1024):.0f} MB\")\n",
    "logger.info(f\"Vectors ready for semantic search!\")\n",
    "logger.info(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Verify Supabase Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify vectors in Supabase\n",
    "logger.info(\"\\nVerifying Supabase...\")\n",
    "\n",
    "try:\n",
    "    response = supabase.table('document_embeddings').select(\"count\", count=\"exact\").execute()\n",
    "    db_vectors = response.count\n",
    "    \n",
    "    logger.info(f\"âœ… Total vectors in Supabase: {db_vectors:,}\")\n",
    "    \n",
    "    if db_vectors == stats['uploaded_vectors']:\n",
    "        logger.info(\"âœ… Upload verification PASSED\")\n",
    "    else:\n",
    "        logger.warning(f\"âš ï¸  Expected {stats['uploaded_vectors']}, but found {db_vectors}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"âŒ Verification failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: (Optional) Create HNSW Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HNSW index for similarity search (if not already created)\n",
    "logger.info(\"\\nNote: HNSW index creation happens in Supabase SQL Editor\")\n",
    "logger.info(\"Run this SQL to create index (one-time):\")\n",
    "logger.info(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS idx_embedding_hnsw\n",
    "ON document_embeddings\n",
    "USING hnsw (embedding vector_cosine_ops)\n",
    "WITH (m = 16, ef_construction = 64);\n",
    "\"\"\")\n",
    "logger.info(\"Index creation takes 30-60 min for 600k vectors (one-time cost)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: (Optional) Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similarity search (if vectors exist)\n",
    "if stats['uploaded_vectors'] > 0:\n",
    "    logger.info(\"\\nTesting similarity search...\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_text = \"Cum se calculeazÄƒ aria unui pÄƒtrat?\"  # Example Romanian query\n",
    "    query_emb = embedding_model.encode([query_text])[0].tolist()\n",
    "    \n",
    "    try:\n",
    "        # Use RPC function (defined in SQL)\n",
    "        results = supabase.rpc('match_documents', {\n",
    "            'query_embedding': query_emb,\n",
    "            'match_count': 5\n",
    "        }).execute()\n",
    "        \n",
    "        logger.info(f\"\\nQuery: '{query_text}'\")\n",
    "        logger.info(f\"Results:\")\n",
    "        \n",
    "        if results.data:\n",
    "            for i, result in enumerate(results.data[:3]):\n",
    "                text = result.get('text', '')[:60]\n",
    "                similarity = result.get('similarity', 0)\n",
    "                logger.info(f\"  {i+1}. {text}... (similarity: {similarity:.1%})\")\n",
    "        else:\n",
    "            logger.info(\"  No results found (index might not be created yet)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Query failed: {e}\")\n",
    "        logger.info(\"(This is OK if RPC function not created yet)\")\n",
    "else:\n",
    "    logger.info(\"No vectors uploaded, skipping search test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"NEXT STEPS\")\n",
    "logger.info(\"=\"*70)\n",
    "logger.info(\"\"\"\n",
    "1. âœ… Embeddings are in Supabase\n",
    "2. âœ… Run SQL to create HNSW index (Supabase SQL Editor)\n",
    "3. âœ… Wait 30-60 min for index to build\n",
    "4. âœ… Now ready to use in AI tutoring system!\n",
    "\n",
    "Your vectors are permanent in Supabase free tier.\n",
    "Storage used: ~300-500 MB / 500 MB available\n",
    "\n",
    "For your AI app:\n",
    "- Generate query embedding (same model)\n",
    "- Call Supabase RPC: match_documents()\n",
    "- Get top-10 similar chunks\n",
    "- Feed to LLM for context\n",
    "\"\"\")\n",
    "logger.info(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
