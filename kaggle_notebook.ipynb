{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö PDF-to-Embedding Pipeline\n",
    "## Procesare 15GB Manuale »òcolare ‚Üí Supabase pgvector\n",
    "\n",
    "**‚ö†Ô∏è  IMPORTANT: RuleazƒÉ celulele √éN ORDINE (1 ‚Üí 2 ‚Üí 3 ‚Üí 4 ‚Üí 5 ‚Üí 6)**\n",
    "\n",
    "**Configurare Kaggle necesarƒÉ:**\n",
    "1. ‚úÖ **Accelerator**: GPU P100 (Settings ‚Üí Accelerator ‚Üí GPU)\n",
    "2. ‚úÖ **Internet**: ON (Settings ‚Üí Internet ‚Üí ON)\n",
    "3. ‚úÖ **Secrets**: Add in Settings ‚Üí Secrets:\n",
    "   - `SUPABASE_URL` = your-project-url.supabase.co\n",
    "   - `SUPABASE_ANON_KEY` = eyJhbGc... (anon public key)\n",
    "4. ‚úÖ **Dataset**: Upload `materiale_didactice.zip` ca Kaggle Dataset\n",
    "\n",
    "**Timp estimat:** 18-25 ore pentru 15GB PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Verificare GPU + Instalare Dependen»õe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificare GPU disponibil\n",
    "print(\"=\" * 70)\n",
    "print(\"VERIFICARE GPU\")\n",
    "print(\"=\" * 70)\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INSTALARE DEPENDEN»öE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Instalare toate dependen»õele necesare\n",
    "!pip install -q PyMuPDF>=1.23.0 \\\n",
    "    sentence-transformers>=2.2.2 \\\n",
    "    supabase>=2.0.0 \\\n",
    "    numpy>=1.24.0 \\\n",
    "    tqdm>=4.65.0 \\\n",
    "    pyyaml>=6.0\n",
    "\n",
    "# PaddleOCR - OP»öIONAL (comenteazƒÉ dacƒÉ nu vrei OCR)\n",
    "# OCR adaugƒÉ timp de procesare dar extrage text din imagini\n",
    "ENABLE_OCR = False  # SchimbƒÉ √Æn True pentru OCR\n",
    "\n",
    "if ENABLE_OCR:\n",
    "    print(\"\\nInstalare PaddleOCR...\")\n",
    "    !pip install -q paddleocr>=2.7.0\n",
    "    print(\"‚úÖ PaddleOCR instalat\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  OCR DISABLED - doar text extraction simplu\")\n",
    "    print(\"    (SeteazƒÉ ENABLE_OCR=True pentru OCR)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TOATE DEPENDEN»öELE INSTALATE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Import Module + Configurare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import hashlib\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Import third-party libraries\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from supabase import create_client\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Import PaddleOCR dacƒÉ activat\n",
    "if ENABLE_OCR:\n",
    "    from paddleocr import PaddleOCR\n",
    "    print(\"‚úÖ PaddleOCR importat\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"√éNCƒÇRCARE CREDEN»öIALE SUPABASE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ob»õine Supabase credentials din Kaggle Secrets\n",
    "secret = UserSecretsClient()\n",
    "\n",
    "try:\n",
    "    SUPABASE_URL = secret.get_secret('SUPABASE_URL')\n",
    "    SUPABASE_KEY = secret.get_secret('SUPABASE_ANON_KEY')\n",
    "    logger.info(f\"‚úÖ Credentials loaded: {SUPABASE_URL[:40]}...\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå EROARE: Nu pot √ÆncƒÉrca secrets!\")\n",
    "    logger.error(f\"   {e}\")\n",
    "    logger.error(\"\\nAsigurƒÉ-te cƒÉ ai adƒÉugat √Æn Settings ‚Üí Secrets:\")\n",
    "    logger.error(\"   - SUPABASE_URL\")\n",
    "    logger.error(\"   - SUPABASE_ANON_KEY\")\n",
    "    raise\n",
    "\n",
    "# Configurare procesare\n",
    "CONFIG = {\n",
    "    'chunk_size': 500,\n",
    "    'chunk_overlap': 50,\n",
    "    'min_chunk_length': 50,\n",
    "    'embedding_model': 'paraphrase-multilingual-mpnet-base-v2',\n",
    "    'embedding_batch_size': 128,\n",
    "    'embedding_device': 'cuda',  # GPU\n",
    "    'supabase_batch_size': 5000,  # 5k vectors per batch\n",
    "    'max_retries': 3,\n",
    "    'ocr_enabled': ENABLE_OCR,\n",
    "    'ocr_languages': ['ro', 'en'],\n",
    "    'ocr_min_confidence': 0.6,\n",
    "    'test_mode': False  # ‚ö†Ô∏è SCHIMBƒÇ √éN True pentru testare rapidƒÉ (10 PDFs)\n",
    "}\n",
    "\n",
    "logger.info(\"\\n\" + \"=\" * 70)\n",
    "logger.info(\"CONFIGURARE\")\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(f\"Chunk size: {CONFIG['chunk_size']} caractere\")\n",
    "logger.info(f\"Overlap: {CONFIG['chunk_overlap']} caractere\")\n",
    "logger.info(f\"Embedding model: {CONFIG['embedding_model']}\")\n",
    "logger.info(f\"Device: {CONFIG['embedding_device']}\")\n",
    "logger.info(f\"Batch upload: {CONFIG['supabase_batch_size']} vectors\")\n",
    "logger.info(f\"OCR enabled: {CONFIG['ocr_enabled']}\")\n",
    "logger.info(f\"Test mode: {CONFIG['test_mode']} {'(doar 10 PDFs!)' if CONFIG['test_mode'] else '(toate PDFs)'}\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ Import »ôi configurare complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: GƒÉsire PDFs √Æn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CƒÇUTARE PDFs √éN DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Localizare dataset folder √Æn Kaggle\n",
    "kaggle_input = Path('/kaggle/input')\n",
    "\n",
    "# Listare toate dataset-urile disponibile\n",
    "dataset_folders = [d for d in kaggle_input.iterdir() if d.is_dir()]\n",
    "logger.info(f\"\\nDataset-uri gƒÉsite √Æn /kaggle/input/:\")\n",
    "for folder in dataset_folders:\n",
    "    logger.info(f\"  - {folder.name}\")\n",
    "\n",
    "if not dataset_folders:\n",
    "    logger.error(\"\\n‚ùå EROARE: Nu existƒÉ dataset-uri √Æn /kaggle/input/\")\n",
    "    logger.error(\"\\nTrebuie sƒÉ:\")\n",
    "    logger.error(\"  1. Uploadezi materiale_didactice.zip ca Kaggle Dataset\")\n",
    "    logger.error(\"  2. Ata»ô dataset-ul la acest notebook (Add Data ‚Üí Your Datasets)\")\n",
    "    logger.error(\"  3. Restart notebook dupƒÉ ata»ôare\")\n",
    "    raise FileNotFoundError(\"Dataset lipse»ôte!\")\n",
    "\n",
    "# Folose»ôte primul dataset (sau specificƒÉ manual)\n",
    "pdf_folder = dataset_folders[0]\n",
    "logger.info(f\"\\n‚úÖ Folosesc dataset: {pdf_folder.name}\")\n",
    "\n",
    "# GƒÉse»ôte toate PDFs recursiv\n",
    "logger.info(\"\\nCƒÉutare PDFs recursiv...\")\n",
    "all_pdfs = list(pdf_folder.glob('**/*.pdf'))\n",
    "\n",
    "if not all_pdfs:\n",
    "    logger.error(\"\\n‚ùå EROARE: Nu s-au gƒÉsit PDFs √Æn dataset!\")\n",
    "    logger.error(f\"   VerificƒÉ cƒÉ dataset-ul con»õine foldere cu PDFs\")\n",
    "    raise FileNotFoundError(\"PDFs lipsesc din dataset!\")\n",
    "\n",
    "logger.info(f\"\\n\" + \"=\" * 70)\n",
    "logger.info(f\"‚úÖ GƒÇSITE {len(all_pdfs)} PDFs\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "# Afi»ôare sample PDFs\n",
    "logger.info(\"\\nPrime 10 PDFs:\")\n",
    "for i, pdf in enumerate(all_pdfs[:10]):\n",
    "    size_mb = pdf.stat().st_size / 1024 / 1024\n",
    "    relative_path = pdf.relative_to(pdf_folder)\n",
    "    logger.info(f\"  {i+1}. {relative_path} ({size_mb:.1f} MB)\")\n",
    "\n",
    "if len(all_pdfs) > 10:\n",
    "    logger.info(f\"  ... »ôi √ÆncƒÉ {len(all_pdfs) - 10} PDFs\")\n",
    "\n",
    "# Calculare dimensiune totalƒÉ\n",
    "total_size_bytes = sum(p.stat().st_size for p in all_pdfs)\n",
    "total_size_gb = total_size_bytes / 1024 / 1024 / 1024\n",
    "\n",
    "logger.info(f\"\\nDimensiune totalƒÉ dataset: {total_size_gb:.2f} GB\")\n",
    "logger.info(f\"Timp estimat procesare: {len(all_pdfs) * 3 / 60:.0f}-{len(all_pdfs) * 5 / 60:.0f} ore\")\n",
    "\n",
    "print(\"\\n‚úÖ PDFs gƒÉsite »ôi verificate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Definire Func»õii de Procesare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DEFINIRE FUNC»öII DE PROCESARE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================================================================\n",
    "# 1. PDF TEXT EXTRACTION\n",
    "# ===================================================================\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Extrage text din PDF folosind PyMuPDF.\n",
    "    \n",
    "    Returns:\n",
    "        dict cu 'text', 'pages', 'images', 'status'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\"\n",
    "        images_found = []\n",
    "        \n",
    "        for page_num, page in enumerate(doc):\n",
    "            # Extract text\n",
    "            page_text = page.get_text()\n",
    "            full_text += page_text + \"\\n\"\n",
    "            \n",
    "            # Detect images (op»õional - pentru OCR)\n",
    "            if CONFIG['ocr_enabled']:\n",
    "                image_list = page.get_images(full=True)\n",
    "                for img in image_list:\n",
    "                    images_found.append({\n",
    "                        'page': page_num + 1,\n",
    "                        'xref': img[0]\n",
    "                    })\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        return {\n",
    "            'text': full_text,\n",
    "            'pages': len(doc),\n",
    "            'images': images_found,\n",
    "            'status': 'success',\n",
    "            'char_count': len(full_text)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'text': '',\n",
    "            'pages': 0,\n",
    "            'images': [],\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# ===================================================================\n",
    "# 2. OCR PROCESSING (OP»öIONAL)\n",
    "# ===================================================================\n",
    "\n",
    "def extract_text_from_images_ocr(pdf_path: Path, images: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Extrage text din imagini folosind PaddleOCR.\n",
    "    RuleazƒÉ doar dacƒÉ OCR activat.\n",
    "    \n",
    "    Returns:\n",
    "        str - text extras din imagini\n",
    "    \"\"\"\n",
    "    if not CONFIG['ocr_enabled']:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Ini»õializare OCR (lazy loading)\n",
    "        if not hasattr(extract_text_from_images_ocr, 'ocr'):\n",
    "            logger.info(\"Ini»õializare PaddleOCR...\")\n",
    "            extract_text_from_images_ocr.ocr = PaddleOCR(\n",
    "                use_angle_cls=True,\n",
    "                lang='en',  # Ro + En\n",
    "                use_gpu=True,\n",
    "                show_log=False\n",
    "            )\n",
    "        \n",
    "        ocr = extract_text_from_images_ocr.ocr\n",
    "        ocr_text = \"\"\n",
    "        \n",
    "        # Process doar primele 5 imagini (pentru vitezƒÉ)\n",
    "        for img_info in images[:5]:\n",
    "            try:\n",
    "                # Extract image din PDF\n",
    "                doc = fitz.open(pdf_path)\n",
    "                page = doc[img_info['page'] - 1]\n",
    "                pix = page.get_pixmap()\n",
    "                img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(\n",
    "                    pix.height, pix.width, pix.n\n",
    "                )\n",
    "                doc.close()\n",
    "                \n",
    "                # Run OCR\n",
    "                result = ocr.ocr(img_array, cls=True)\n",
    "                \n",
    "                if result and result[0]:\n",
    "                    for line in result[0]:\n",
    "                        text = line[1][0]\n",
    "                        confidence = line[1][1]\n",
    "                        if confidence >= CONFIG['ocr_min_confidence']:\n",
    "                            ocr_text += text + \" \"\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return ocr_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"OCR failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ===================================================================\n",
    "# 3. TEXT CHUNKING\n",
    "# ===================================================================\n",
    "\n",
    "def chunk_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    √émparte text √Æn chunks cu overlap.\n",
    "    \n",
    "    Returns:\n",
    "        List[str] - lista de chunks\n",
    "    \"\"\"\n",
    "    chunk_size = CONFIG['chunk_size']\n",
    "    overlap = CONFIG['chunk_overlap']\n",
    "    min_length = CONFIG['min_chunk_length']\n",
    "    \n",
    "    # Split pe sentin»õe (aproximativ)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= chunk_size:\n",
    "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "        else:\n",
    "            if len(current_chunk) >= min_length:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    \n",
    "    # AdaugƒÉ ultimul chunk\n",
    "    if len(current_chunk) >= min_length:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# ===================================================================\n",
    "# 4. DEDUPLICATION\n",
    "# ===================================================================\n",
    "\n",
    "def deduplicate_chunks(chunks: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    EliminƒÉ duplicate chunks (headers/footers repetate).\n",
    "    \n",
    "    Returns:\n",
    "        List[str] - chunks unice\n",
    "    \"\"\"\n",
    "    seen_hashes = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_hash = hashlib.md5(chunk.encode('utf-8')).hexdigest()\n",
    "        \n",
    "        if chunk_hash not in seen_hashes:\n",
    "            seen_hashes.add(chunk_hash)\n",
    "            unique_chunks.append(chunk)\n",
    "    \n",
    "    return unique_chunks\n",
    "\n",
    "# ===================================================================\n",
    "# 5. EMBEDDING GENERATION\n",
    "# ===================================================================\n",
    "\n",
    "def init_embedding_model():\n",
    "    \"\"\"\n",
    "    Ini»õializare sentence-transformers model.\n",
    "    \"\"\"\n",
    "    logger.info(f\"√éncƒÉrcare model: {CONFIG['embedding_model']}\")\n",
    "    logger.info(f\"Device: {CONFIG['embedding_device']}\")\n",
    "    \n",
    "    model = SentenceTransformer(\n",
    "        CONFIG['embedding_model'],\n",
    "        device=CONFIG['embedding_device']\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"‚úÖ Model √ÆncƒÉrcat: 768 dimensiuni\")\n",
    "    return model\n",
    "\n",
    "def generate_embeddings(texts: List[str], model) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GenereazƒÉ embeddings pentru listƒÉ de texte.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray shape (N, 768)\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=CONFIG['embedding_batch_size'],\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "# ===================================================================\n",
    "# 6. SUPABASE UPLOAD\n",
    "# ===================================================================\n",
    "\n",
    "def init_supabase():\n",
    "    \"\"\"\n",
    "    Ini»õializare Supabase client.\n",
    "    \"\"\"\n",
    "    client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    logger.info(\"‚úÖ Supabase client ini»õializat\")\n",
    "    return client\n",
    "\n",
    "def upload_vectors_to_supabase(supabase, vectors: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Upload vectors √Æn Supabase cu batching.\n",
    "    \n",
    "    Returns:\n",
    "        dict cu 'success' »ôi 'failed' counts\n",
    "    \"\"\"\n",
    "    batch_size = CONFIG['supabase_batch_size']\n",
    "    max_retries = CONFIG['max_retries']\n",
    "    \n",
    "    total_success = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        batch = vectors[i:i + batch_size]\n",
    "        \n",
    "        # Retry logic\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = supabase.table('document_embeddings').insert(batch).execute()\n",
    "                total_success += len(batch)\n",
    "                break  # Success, exit retry loop\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    logger.warning(f\"Batch {i//batch_size} failed after {max_retries} retries: {e}\")\n",
    "                    total_failed += len(batch)\n",
    "                else:\n",
    "                    time.sleep(2)  # Wait before retry\n",
    "    \n",
    "    return {'success': total_success, 'failed': total_failed}\n",
    "\n",
    "logger.info(\"\\n‚úÖ Toate func»õiile definite!\")\n",
    "logger.info(\"   - extract_text_from_pdf()\")\n",
    "logger.info(\"   - extract_text_from_images_ocr() [op»õional]\")\n",
    "logger.info(\"   - chunk_text()\")\n",
    "logger.info(\"   - deduplicate_chunks()\")\n",
    "logger.info(\"   - init_embedding_model()\")\n",
    "logger.info(\"   - generate_embeddings()\")\n",
    "logger.info(\"   - upload_vectors_to_supabase()\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Ini»õializare Componente (Model + Supabase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"INI»öIALIZARE COMPONENTE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ini»õializare embedding model\n",
    "logger.info(\"\\n√éncarc embedding model pe GPU...\")\n",
    "embedding_model = init_embedding_model()\n",
    "\n",
    "# Ini»õializare Supabase\n",
    "logger.info(\"\\nConectare la Supabase...\")\n",
    "supabase = init_supabase()\n",
    "\n",
    "# Testare conexiune Supabase\n",
    "try:\n",
    "    test_response = supabase.table('document_embeddings').select('count', count='exact').limit(1).execute()\n",
    "    current_count = test_response.count if test_response.count else 0\n",
    "    logger.info(f\"‚úÖ Supabase conectat - {current_count} vectors existen»õi √Æn DB\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Eroare testare conexiune Supabase: {e}\")\n",
    "    logger.error(\"VerificƒÉ cƒÉ ai rulat supabase_setup.sql √Æn SQL Editor!\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ COMPONENTE INI»öIALIZATE - GATA DE PROCESARE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: PIPELINE PRINCIPAL DE PROCESARE\n",
    "\n",
    "**‚ö†Ô∏è  IMPORTANT**: AceastƒÉ celulƒÉ depinde de celulele anterioare!\n",
    "- Cell 2: `CONFIG`\n",
    "- Cell 3: `all_pdfs`, `pdf_folder`\n",
    "- Cell 5: `embedding_model`, `supabase`\n",
    "\n",
    "DacƒÉ prime»ôti `NameError`, ruleazƒÉ celulele 1-5 √Ænainte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"√éNCEPE PROCESAREA PRINCIPALƒÇ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================================================================\n",
    "# VERIFICARE DEPENDEN»öE DIN CELULELE ANTERIOARE\n",
    "# ===================================================================\n",
    "required_vars = {\n",
    "    'all_pdfs': 'Cell 3 (GƒÉsire PDFs)',\n",
    "    'pdf_folder': 'Cell 3 (GƒÉsire PDFs)',\n",
    "    'embedding_model': 'Cell 5 (Ini»õializare Componente)',\n",
    "    'supabase': 'Cell 5 (Ini»õializare Componente)',\n",
    "    'CONFIG': 'Cell 2 (Configurare)'\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "for var_name, source_cell in required_vars.items():\n",
    "    if var_name not in globals():\n",
    "        missing_vars.append(f\"  ‚ùå {var_name} (din {source_cell})\")\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ùå EROARE: Lipsesc variabile necesare!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nRuleazƒÉ celulele √Æn ordine √éNAINTE de aceastƒÉ celulƒÉ:\\n\")\n",
    "    for missing in missing_vars:\n",
    "        print(missing)\n",
    "    print(\"\\nPa≈üi de rezolvare:\")\n",
    "    print(\"  1. RuleazƒÉ Cell 1 (Instalare Dependen»õe)\")\n",
    "    print(\"  2. RuleazƒÉ Cell 2 (Configurare)\")\n",
    "    print(\"  3. RuleazƒÉ Cell 3 (GƒÉsire PDFs)\")\n",
    "    print(\"  4. RuleazƒÉ Cell 4 (Definire Func»õii)\")\n",
    "    print(\"  5. RuleazƒÉ Cell 5 (Ini»õializare Componente)\")\n",
    "    print(\"  6. Apoi ruleazƒÉ aceastƒÉ celulƒÉ\\n\")\n",
    "    raise NameError(f\"Variabile lipsƒÉ: {', '.join([v.split(' ')[1] for v in missing_vars])}\")\n",
    "\n",
    "print(\"‚úÖ Toate dependen»õele verificate - continuƒÉm procesarea...\\n\")\n",
    "\n",
    "# ===================================================================\n",
    "# MOD TEST vs COMPLET (din CONFIG)\n",
    "# ===================================================================\n",
    "if CONFIG['test_mode']:\n",
    "    pdfs_to_process = all_pdfs[:10]\n",
    "    logger.warning(\"\\n‚ö†Ô∏è  MOD TEST ACTIVAT - se proceseazƒÉ doar 10 PDFs!\")\n",
    "    logger.warning(\"   SchimbƒÉ CONFIG['test_mode']=False √Æn Cell 2 pentru procesare completƒÉ\\n\")\n",
    "else:\n",
    "    pdfs_to_process = all_pdfs\n",
    "    logger.info(f\"\\nMod COMPLET - se proceseazƒÉ toate {len(all_pdfs)} PDFs\\n\")\n",
    "\n",
    "# Ini»õializare statistici\n",
    "stats = {\n",
    "    'start_time': time.time(),\n",
    "    'processed_pdfs': 0,\n",
    "    'failed_pdfs': 0,\n",
    "    'total_pages': 0,\n",
    "    'total_chars': 0,\n",
    "    'total_chunks': 0,\n",
    "    'total_unique_chunks': 0,\n",
    "    'total_embeddings': 0,\n",
    "    'uploaded_vectors': 0,\n",
    "    'failed_uploads': 0,\n",
    "    'ocr_images_processed': 0\n",
    "}\n",
    "\n",
    "# Buffer pentru batch upload\n",
    "upload_buffer = []\n",
    "buffer_max_size = CONFIG['supabase_batch_size']\n",
    "\n",
    "# ===================================================================\n",
    "# MAIN PROCESSING LOOP\n",
    "# ===================================================================\n",
    "\n",
    "logger.info(\"√éncepe procesare...\\n\")\n",
    "\n",
    "with tqdm(total=len(pdfs_to_process), desc=\"üìÑ Procesare PDFs\") as pbar:\n",
    "    \n",
    "    for pdf_path in pdfs_to_process:\n",
    "        try:\n",
    "            # ============================================================\n",
    "            # STEP 1: EXTRAGERE TEXT DIN PDF\n",
    "            # ============================================================\n",
    "            result = extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            if result['status'] != 'success':\n",
    "                logger.warning(f\"Failed: {pdf_path.name} - {result.get('error', 'Unknown error')}\")\n",
    "                stats['failed_pdfs'] += 1\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            text = result['text']\n",
    "            pages = result['pages']\n",
    "            images = result['images']\n",
    "            \n",
    "            stats['processed_pdfs'] += 1\n",
    "            stats['total_pages'] += pages\n",
    "            stats['total_chars'] += len(text)\n",
    "            \n",
    "            # ============================================================\n",
    "            # STEP 2: OCR PE IMAGINI (dacƒÉ activat)\n",
    "            # ============================================================\n",
    "            ocr_text = \"\"\n",
    "            if CONFIG['ocr_enabled'] and images:\n",
    "                ocr_text = extract_text_from_images_ocr(pdf_path, images)\n",
    "                if ocr_text:\n",
    "                    text = text + \"\\n\" + ocr_text\n",
    "                    stats['ocr_images_processed'] += len(images)\n",
    "            \n",
    "            # Skip PDFs fƒÉrƒÉ text\n",
    "            if len(text.strip()) < 100:\n",
    "                logger.warning(f\"Skipping {pdf_path.name} - prea pu»õin text ({len(text)} chars)\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # ============================================================\n",
    "            # STEP 3: CHUNKING TEXT\n",
    "            # ============================================================\n",
    "            chunks = chunk_text(text)\n",
    "            stats['total_chunks'] += len(chunks)\n",
    "            \n",
    "            # ============================================================\n",
    "            # STEP 4: DEDUPLICARE CHUNKS\n",
    "            # ============================================================\n",
    "            unique_chunks = deduplicate_chunks(chunks)\n",
    "            stats['total_unique_chunks'] += len(unique_chunks)\n",
    "            \n",
    "            # ============================================================\n",
    "            # STEP 5: GENERARE EMBEDDINGS\n",
    "            # ============================================================\n",
    "            embeddings = generate_embeddings(unique_chunks, embedding_model)\n",
    "            stats['total_embeddings'] += len(embeddings)\n",
    "            \n",
    "            # ============================================================\n",
    "            # STEP 6: PREGƒÇTIRE PENTRU UPLOAD\n",
    "            # ============================================================\n",
    "            pdf_relative_path = str(pdf_path.relative_to(pdf_folder))\n",
    "            \n",
    "            # Extract metadata din path (ex: clasa_2/matematica/...)\n",
    "            path_parts = pdf_relative_path.split(os.sep)\n",
    "            clasa = 0\n",
    "            materie = \"General\"\n",
    "            \n",
    "            # √éncearcƒÉ sƒÉ extragi clasa din path (ex: clasa_2)\n",
    "            for part in path_parts:\n",
    "                if 'clasa' in part.lower():\n",
    "                    try:\n",
    "                        clasa = int(re.search(r'\\d+', part).group())\n",
    "                    except:\n",
    "                        pass\n",
    "                # Materia poate fi numele folderului\n",
    "                if len(path_parts) > 1:\n",
    "                    materie = path_parts[1] if len(path_parts) > 2 else path_parts[0]\n",
    "            \n",
    "            # CreeazƒÉ vector items pentru Supabase\n",
    "            for idx, (chunk, embedding) in enumerate(zip(unique_chunks, embeddings)):\n",
    "                chunk_hash = hashlib.md5(chunk.encode('utf-8')).hexdigest()\n",
    "                \n",
    "                vector_item = {\n",
    "                    'chunk_id': f\"{pdf_path.stem}_{idx}_{chunk_hash[:8]}\",\n",
    "                    'text': chunk[:10000],  # LimitƒÉ 10k chars pentru DB\n",
    "                    'embedding': embedding.tolist(),\n",
    "                    'source_pdf': pdf_relative_path,\n",
    "                    'page_num': 1,  # Simplificat - nu trackuim pagina exactƒÉ\n",
    "                    'clasa': clasa,\n",
    "                    'materie': materie,\n",
    "                    'capitol': 'General',\n",
    "                    'chunk_hash': chunk_hash,\n",
    "                    'has_images': len(images) > 0\n",
    "                }\n",
    "                \n",
    "                upload_buffer.append(vector_item)\n",
    "            \n",
    "            # ============================================================\n",
    "            # STEP 7: UPLOAD BATCH C√ÇND BUFFER PLIN\n",
    "            # ============================================================\n",
    "            if len(upload_buffer) >= buffer_max_size:\n",
    "                upload_result = upload_vectors_to_supabase(supabase, upload_buffer)\n",
    "                stats['uploaded_vectors'] += upload_result['success']\n",
    "                stats['failed_uploads'] += upload_result['failed']\n",
    "                upload_buffer = []  # Clear buffer\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Eroare criticƒÉ procesare {pdf_path.name}: {e}\")\n",
    "            stats['failed_pdfs'] += 1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "# ===================================================================\n",
    "# UPLOAD ULTIMUL BATCH RƒÇMAS √éN BUFFER\n",
    "# ===================================================================\n",
    "if upload_buffer:\n",
    "    logger.info(f\"\\nUpload ultimul batch: {len(upload_buffer)} vectors...\")\n",
    "    upload_result = upload_vectors_to_supabase(supabase, upload_buffer)\n",
    "    stats['uploaded_vectors'] += upload_result['success']\n",
    "    stats['failed_uploads'] += upload_result['failed']\n",
    "\n",
    "stats['end_time'] = time.time()\n",
    "stats['elapsed_seconds'] = stats['end_time'] - stats['start_time']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ PROCESARE COMPLETƒÇ\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Afi»ôare Statistici Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä STATISTICI FINALE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculare timp\n",
    "elapsed_hours = stats['elapsed_seconds'] / 3600\n",
    "elapsed_minutes = (stats['elapsed_seconds'] % 3600) / 60\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  TIMP PROCESARE\")\n",
    "print(f\"   Total: {elapsed_hours:.1f} ore ({elapsed_minutes:.0f} minute)\")\n",
    "print(f\"   Timp mediu per PDF: {stats['elapsed_seconds'] / max(stats['processed_pdfs'], 1):.1f} secunde\")\n",
    "\n",
    "print(f\"\\nüìÑ PDFs\")\n",
    "print(f\"   ‚úÖ Procesate cu succes: {stats['processed_pdfs']:,}\")\n",
    "print(f\"   ‚ùå Failed: {stats['failed_pdfs']:,}\")\n",
    "print(f\"   üìë Total pagini: {stats['total_pages']:,}\")\n",
    "\n",
    "print(f\"\\nüìù TEXT\")\n",
    "print(f\"   Total caractere: {stats['total_chars']:,}\")\n",
    "print(f\"   Mediu per PDF: {stats['total_chars'] // max(stats['processed_pdfs'], 1):,} caractere\")\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è  CHUNKING\")\n",
    "print(f\"   Total chunks create: {stats['total_chunks']:,}\")\n",
    "print(f\"   DupƒÉ deduplicare: {stats['total_unique_chunks']:,}\")\n",
    "print(f\"   Duplicate eliminate: {stats['total_chunks'] - stats['total_unique_chunks']:,}\")\n",
    "print(f\"   Mediu chunks per PDF: {stats['total_unique_chunks'] // max(stats['processed_pdfs'], 1)}\")\n",
    "\n",
    "print(f\"\\nüß† EMBEDDINGS\")\n",
    "print(f\"   Total vectors generate: {stats['total_embeddings']:,}\")\n",
    "print(f\"   Dimensiune: 768 (paraphrase-multilingual-mpnet-base-v2)\")\n",
    "\n",
    "if CONFIG['ocr_enabled']:\n",
    "    print(f\"\\nüîç OCR\")\n",
    "    print(f\"   Imagini procesate: {stats['ocr_images_processed']:,}\")\n",
    "\n",
    "print(f\"\\n‚òÅÔ∏è  SUPABASE UPLOAD\")\n",
    "print(f\"   ‚úÖ Uploaded cu succes: {stats['uploaded_vectors']:,} vectors\")\n",
    "print(f\"   ‚ùå Failed uploads: {stats['failed_uploads']:,}\")\n",
    "\n",
    "# Estimare dimensiune DB\n",
    "estimated_db_mb = (stats['uploaded_vectors'] * 3.5) / 1024  # ~3.5KB per vector\n",
    "print(f\"\\nüíæ DIMENSIUNE DATABASE\")\n",
    "print(f\"   Estimat: ~{estimated_db_mb:.0f} MB\")\n",
    "print(f\"   Supabase free tier: 500 MB\")\n",
    "if estimated_db_mb < 500:\n",
    "    print(f\"   ‚úÖ √én limitƒÉ ({500 - estimated_db_mb:.0f} MB rƒÉmase)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  DEPƒÇ»òE»òTE limita cu {estimated_db_mb - 500:.0f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ PROCESARE FINALIZATƒÇ CU SUCCES!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Verificare Supabase Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîç VERIFICARE SUPABASE DATABASE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Count total vectors\n",
    "    response = supabase.table('document_embeddings').select('count', count='exact').limit(1).execute()\n",
    "    db_count = response.count if response.count else 0\n",
    "    \n",
    "    print(f\"\\nüìä Vectors √Æn database: {db_count:,}\")\n",
    "    print(f\"   Uploaded √Æn acest run: {stats['uploaded_vectors']:,}\")\n",
    "    \n",
    "    if db_count == stats['uploaded_vectors']:\n",
    "        print(\"   ‚úÖ VERIFICARE PASSED - toate vectorii uploada»õi\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Diferen»õƒÉ: {abs(db_count - stats['uploaded_vectors']):,} vectors\")\n",
    "        print(\"      (Poate include vectors din rulƒÉri anterioare)\")\n",
    "    \n",
    "    # Sample query - afi»ôeazƒÉ primele 3 records\n",
    "    print(\"\\nüìÑ Sample records din database:\")\n",
    "    sample = supabase.table('document_embeddings').select('chunk_id, source_pdf, clasa, materie').limit(3).execute()\n",
    "    \n",
    "    if sample.data:\n",
    "        for i, record in enumerate(sample.data, 1):\n",
    "            print(f\"   {i}. {record['chunk_id']}\")\n",
    "            print(f\"      PDF: {record['source_pdf']}\")\n",
    "            print(f\"      Clasa: {record['clasa']}, Materie: {record['materie']}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Nu s-au gƒÉsit records\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Verificare completƒÉ - database OK!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Eroare verificare database: {e}\")\n",
    "    print(\"   VerificƒÉ conexiunea Supabase »ôi tabelul 'document_embeddings'\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Test Similarity Search (Op»õional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîç TEST SIMILARITY SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if stats['uploaded_vectors'] == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  Nu existƒÉ vectors √Æn database - skip search test\")\n",
    "else:\n",
    "    try:\n",
    "        # Query de test √Æn rom√¢nƒÉ\n",
    "        test_queries = [\n",
    "            \"Cum se calculeazƒÉ aria unui pƒÉtrat?\",\n",
    "            \"Ce este fotosinteza?\",\n",
    "            \"Capitala Rom√¢niei\"\n",
    "        ]\n",
    "        \n",
    "        for query_text in test_queries:\n",
    "            print(f\"\\nüìù Query: '{query_text}'\")\n",
    "            \n",
    "            # Generate query embedding\n",
    "            query_embedding = embedding_model.encode([query_text])[0].tolist()\n",
    "            \n",
    "            # Call Supabase RPC function\n",
    "            try:\n",
    "                results = supabase.rpc('match_documents', {\n",
    "                    'query_embedding': query_embedding,\n",
    "                    'match_count': 3\n",
    "                }).execute()\n",
    "                \n",
    "                if results.data:\n",
    "                    print(\"   Rezultate:\")\n",
    "                    for i, result in enumerate(results.data, 1):\n",
    "                        text_preview = result.get('text', '')[:80]\n",
    "                        similarity = result.get('similarity', 0)\n",
    "                        source = result.get('source_pdf', 'unknown')\n",
    "                        print(f\"      {i}. [{similarity:.1%}] {text_preview}...\")\n",
    "                        print(f\"         Source: {source}\")\n",
    "                else:\n",
    "                    print(\"   ‚ö†Ô∏è  Nu s-au gƒÉsit rezultate\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Eroare query: {e}\")\n",
    "                print(\"      (VerificƒÉ cƒÉ match_documents() RPC function existƒÉ √Æn Supabase)\")\n",
    "                print(\"      (RuleazƒÉ supabase_setup.sql pentru a crea func»õia)\")\n",
    "                break\n",
    "        \n",
    "        print(\"\\n‚úÖ Similarity search func»õioneazƒÉ!\")\n",
    "        print(\"   Po»õi folosi match_documents() RPC √Æn aplica»õia ta\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Eroare test search: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: NEXT STEPS & CLEANUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ NEXT STEPS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ DONE:\n",
    "   1. PDFs procesate cu succes\n",
    "   2. Embeddings generate (768 dimensiuni)\n",
    "   3. Vectors uploadate √Æn Supabase\n",
    "\n",
    "üîß TODO (dacƒÉ nu ai fƒÉcut deja):\n",
    "   \n",
    "   1. CREATE HNSW INDEX √Æn Supabase SQL Editor:\n",
    "      \n",
    "      CREATE INDEX IF NOT EXISTS idx_embedding_hnsw\n",
    "      ON document_embeddings\n",
    "      USING hnsw (embedding vector_cosine_ops)\n",
    "      WITH (m = 16, ef_construction = 64);\n",
    "      \n",
    "      ‚è±Ô∏è  Timp: 30-60 minute pentru 600k vectors (one-time)\n",
    "   \n",
    "   2. VERIFICƒÇ RPC FUNCTION match_documents() existƒÉ\n",
    "      (Ar trebui creatƒÉ din supabase_setup.sql)\n",
    "\n",
    "üöÄ FOLOSIRE √éN AI TUTORING APP:\n",
    "   \n",
    "   # √én aplica»õia ta Python/Node.js:\n",
    "   \n",
    "   1. User face √Æntrebare: \"Cum calculez aria unui pƒÉtrat?\"\n",
    "   \n",
    "   2. Generate query embedding:\n",
    "      model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "      query_emb = model.encode([question])[0].tolist()\n",
    "   \n",
    "   3. CautƒÉ √Æn Supabase:\n",
    "      results = supabase.rpc('match_documents', {\n",
    "          'query_embedding': query_emb,\n",
    "          'match_count': 10,\n",
    "          'filter_clasa': 2  # Optional\n",
    "      }).execute()\n",
    "   \n",
    "   4. Feed context la LLM:\n",
    "      context = \"\\n\".join([r['text'] for r in results.data])\n",
    "      prompt = f\"Context: {context}\\n\\n√éntrebare: {question}\"\n",
    "      response = openai.ChatCompletion.create(...)\n",
    "\n",
    "üíæ STORAGE:\n",
    "   - Vectors √Æn Supabase: PERMANENT (p√¢nƒÉ la limita de 500MB)\n",
    "   - Po»õi »ôterge acum PDFs locale pentru a elibera 15GB\n",
    "   - Procesarea se face O SINGURƒÇ DATƒÇ\n",
    "\n",
    "üí∞ COST:\n",
    "   - Total: $0 (Kaggle free + Supabase free tier)\n",
    "   - Mentenan»õƒÉ: $0/lunƒÉ\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üéâ SUCCESS! Embeddings gata de folosit √Æn AI tutoring!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
