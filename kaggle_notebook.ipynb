{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Educational - PDF to Supabase Pipeline (Production-Ready)\n",
        "\n",
        "**Bazat pe modulele Python testate local de Edd**\n",
        "\n",
        "- PyMuPDF (fitz) pentru extrac»õie rapidƒÉ\n",
        "- PaddleOCR pentru imagini educa»õionale\n",
        "- Smart chunking cu MD5 deduplication\n",
        "- paraphrase-multilingual-mpnet-base-v2 (768 dim)\n",
        "- Batch upload optimizat Supabase\n",
        "\n",
        "StructurƒÉ input: `/kaggle/input/pdf-files/{clasa}/{materie}/file.pdf`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Verificare Dependen»õe Pre-instalate Kaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "print(f\"Python: {sys.version}\")\n",
        "\n",
        "# Check librƒÉrii critice\n",
        "dependencies_check = {\n",
        "    'fitz (PyMuPDF)': False,\n",
        "    'paddleocr': False,\n",
        "    'sentence_transformers': False,\n",
        "    'supabase': False,\n",
        "    'tqdm': False\n",
        "}\n",
        "\n",
        "try:\n",
        "    import fitz\n",
        "    dependencies_check['fitz (PyMuPDF)'] = True\n",
        "    print(f\"‚úÖ PyMuPDF: {fitz.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå PyMuPDF not found\")\n",
        "\n",
        "try:\n",
        "    import paddleocr\n",
        "    dependencies_check['paddleocr'] = True\n",
        "    print(f\"‚úÖ PaddleOCR installed\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå PaddleOCR not found\")\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    dependencies_check['sentence_transformers'] = True\n",
        "    print(f\"‚úÖ sentence-transformers installed\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå sentence-transformers not found\")\n",
        "\n",
        "try:\n",
        "    import supabase\n",
        "    dependencies_check['supabase'] = True\n",
        "    print(f\"‚úÖ supabase installed\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå supabase not found\")\n",
        "\n",
        "try:\n",
        "    import tqdm\n",
        "    dependencies_check['tqdm'] = True\n",
        "    print(f\"‚úÖ tqdm installed\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå tqdm not found\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Instalare Dependen»õe LipsƒÉ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# InstaleazƒÉ DOAR ce lipse»ôte\n",
        "!pip install -q PyMuPDF==1.23.8\n",
        "!pip install -q paddleocr==2.7.0.3\n",
        "!pip install -q sentence-transformers==2.2.2\n",
        "!pip install -q supabase==2.3.0\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"‚úÖ Toate dependen»õele instalate\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import-uri Principale\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "import hashlib\n",
        "import time\n",
        "import pickle\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# PDF processing\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# OCR\n",
        "from paddleocr import PaddleOCR\n",
        "\n",
        "# Embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Supabase\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"‚úÖ Import-uri complete\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configurare Kaggle Secrets & Supabase\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "user_secrets = UserSecretsClient()\n",
        "\n",
        "SUPABASE_URL = user_secrets.get_secret(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = user_secrets.get_secret(\"SUPABASE_KEY\")\n",
        "\n",
        "# Ini»õializare Supabase client\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "print(\"‚úÖ Supabase conectat\")\n",
        "\n",
        "# Test conexiune\n",
        "try:\n",
        "    response = supabase.table('document_embeddings').select('*').limit(1).execute()\n",
        "    print(f\"‚úÖ Test query OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Test query failed: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configurare Modele (Embeddings + OCR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === EMBEDDING MODEL ===\n",
        "MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'\n",
        "EMBEDDING_DIM = 768\n",
        "\n",
        "print(f\"üì• √éncƒÉrcare embedding model: {MODEL_NAME}\")\n",
        "embedding_model = SentenceTransformer(MODEL_NAME, device='cpu')\n",
        "\n",
        "# Test\n",
        "test_emb = embedding_model.encode([\"Test text\"])\n",
        "assert len(test_emb[0]) == EMBEDDING_DIM\n",
        "print(f\"‚úÖ Embedding model loaded: {EMBEDDING_DIM} dimensiuni\")\n",
        "\n",
        "# === OCR MODEL ===\n",
        "print(f\"üì• √éncƒÉrcare PaddleOCR (ro + en)...\")\n",
        "ocr_model = PaddleOCR(\n",
        "    use_angle_cls=True,\n",
        "    lang=['ro', 'en'],\n",
        "    use_gpu=False,  # CPU mai stabil pe Kaggle\n",
        "    show_log=False\n",
        ")\n",
        "print(f\"‚úÖ PaddleOCR loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Clase & Func»õii - PDF Extraction (PyMuPDF)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ImageData:\n",
        "    \"\"\"Informa»õii despre o imagine din PDF\"\"\"\n",
        "    page_number: int\n",
        "    bbox: Tuple[float, float, float, float]\n",
        "    size_bytes: int\n",
        "    priority_score: float\n",
        "    image_type: str\n",
        "    image_array: Optional[np.ndarray] = None  # Pentru OCR\n",
        "\n",
        "@dataclass\n",
        "class PDFExtractionResult:\n",
        "    \"\"\"Rezultat extrac»õie PDF\"\"\"\n",
        "    pdf_path: str\n",
        "    text: str\n",
        "    images: List[ImageData]\n",
        "    total_pages: int\n",
        "    file_size_bytes: int\n",
        "    metadata: Dict\n",
        "    extraction_status: str\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "def extract_metadata_from_path(pdf_path: str) -> Dict:\n",
        "    \"\"\"Extrage clasa »ôi materia din structura de foldere\"\"\"\n",
        "    path_parts = Path(pdf_path).parts\n",
        "    \n",
        "    metadata = {\n",
        "        'clasa': None,\n",
        "        'materie': None,\n",
        "        'source_pdf': Path(pdf_path).name\n",
        "    }\n",
        "    \n",
        "    # CƒÉutare clasa\n",
        "    for part in path_parts:\n",
        "        if 'clasa' in part.lower() or 'class' in part.lower():\n",
        "            match = re.search(r'\\d+', part)\n",
        "            if match:\n",
        "                metadata['clasa'] = int(match.group())\n",
        "                break\n",
        "    \n",
        "    # CƒÉutare materie\n",
        "    if len(path_parts) >= 2:\n",
        "        metadata['materie'] = path_parts[-2]\n",
        "    \n",
        "    return metadata\n",
        "\n",
        "def calculate_image_priority(pix: fitz.Pixmap, image_size: int) -> float:\n",
        "    \"\"\"Priority score 0.0-1.0 pentru OCR (imagini cu text = prioritate √ÆnaltƒÉ)\"\"\"\n",
        "    score = 0.5\n",
        "    \n",
        "    try:\n",
        "        size_score = min(image_size / 512000, 1.0)\n",
        "        \n",
        "        width = pix.width\n",
        "        height = pix.height\n",
        "        aspect_ratio = max(width, height) / max(min(width, height), 1)\n",
        "        \n",
        "        if 0.8 < aspect_ratio < 1.25:\n",
        "            ratio_score = 0.9  # Aproape pƒÉtrate - diagrame\n",
        "        else:\n",
        "            ratio_score = 0.5\n",
        "        \n",
        "        score = (size_score * 0.4) + (ratio_score * 0.6)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return score\n",
        "\n",
        "def detect_image_type(pix: fitz.Pixmap) -> str:\n",
        "    \"\"\"DetecteazƒÉ tipul imaginii\"\"\"\n",
        "    try:\n",
        "        width = pix.width\n",
        "        height = pix.height\n",
        "        aspect_ratio = max(width, height) / max(min(width, height), 1)\n",
        "        \n",
        "        if width > 1000 or height > 1000:\n",
        "            return \"chart\"\n",
        "        elif width < 200 or height < 200:\n",
        "            return \"diagram\"\n",
        "        elif 0.8 < aspect_ratio < 1.25:\n",
        "            return \"diagram\"\n",
        "        else:\n",
        "            return \"photo\"\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def extract_text_and_images(pdf_path: str) -> PDFExtractionResult:\n",
        "    \"\"\"Extrage text »ôi imagini din PDF cu PyMuPDF\"\"\"\n",
        "    try:\n",
        "        pdf_path = str(pdf_path)\n",
        "        \n",
        "        if not Path(pdf_path).exists():\n",
        "            return PDFExtractionResult(\n",
        "                pdf_path=pdf_path, text=\"\", images=[], total_pages=0,\n",
        "                file_size_bytes=0, metadata={}, extraction_status=\"error\",\n",
        "                error_message=f\"File not found: {pdf_path}\"\n",
        "            )\n",
        "        \n",
        "        pdf_document = fitz.open(pdf_path)\n",
        "        file_size_bytes = Path(pdf_path).stat().st_size\n",
        "        total_pages = len(pdf_document)\n",
        "        \n",
        "        logger.info(f\"Procesare: {Path(pdf_path).name} ({total_pages} pagini)\")\n",
        "        \n",
        "        all_text = []\n",
        "        images_found = []\n",
        "        \n",
        "        for page_number in range(total_pages):\n",
        "            page = pdf_document[page_number]\n",
        "            \n",
        "            # Extrage text\n",
        "            page_text = page.get_text()\n",
        "            all_text.append(page_text)\n",
        "            \n",
        "            # DetecteazƒÉ imagini\n",
        "            image_list = page.get_images()\n",
        "            \n",
        "            for img_index, img_info in enumerate(image_list):\n",
        "                try:\n",
        "                    xref = img_info[0]\n",
        "                    pix = fitz.Pixmap(pdf_document, xref)\n",
        "                    \n",
        "                    image_bytes = pix.tobytes()\n",
        "                    image_size = len(image_bytes)\n",
        "                    \n",
        "                    # FiltreazƒÉ imagini mici (<50KB)\n",
        "                    if image_size < 51200:\n",
        "                        continue\n",
        "                    \n",
        "                    # Bounding box\n",
        "                    image_rect = page.get_image_rects(img_info)\n",
        "                    bbox = image_rect[0] if image_rect else (0, 0, 100, 100)\n",
        "                    \n",
        "                    priority = calculate_image_priority(pix, image_size)\n",
        "                    \n",
        "                    # Converte»ôte pixmap la numpy array pentru OCR\n",
        "                    img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)\n",
        "                    \n",
        "                    img_data = ImageData(\n",
        "                        page_number=page_number + 1,\n",
        "                        bbox=bbox,\n",
        "                        size_bytes=image_size,\n",
        "                        priority_score=priority,\n",
        "                        image_type=detect_image_type(pix),\n",
        "                        image_array=img_array\n",
        "                    )\n",
        "                    \n",
        "                    images_found.append(img_data)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Skip image {img_index} p{page_number+1}: {e}\")\n",
        "                    continue\n",
        "        \n",
        "        combined_text = \"\\n\".join(all_text)\n",
        "        pdf_document.close()\n",
        "        \n",
        "        metadata = extract_metadata_from_path(pdf_path)\n",
        "        \n",
        "        logger.info(f\"‚úÖ Extras: {len(combined_text)} chars, {len(images_found)} imagini\")\n",
        "        \n",
        "        return PDFExtractionResult(\n",
        "            pdf_path=pdf_path,\n",
        "            text=combined_text,\n",
        "            images=images_found,\n",
        "            total_pages=total_pages,\n",
        "            file_size_bytes=file_size_bytes,\n",
        "            metadata=metadata,\n",
        "            extraction_status=\"success\"\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Eroare procesare {pdf_path}: {e}\")\n",
        "        return PDFExtractionResult(\n",
        "            pdf_path=pdf_path, text=\"\", images=[], total_pages=0,\n",
        "            file_size_bytes=0, metadata={}, extraction_status=\"error\",\n",
        "            error_message=str(e)\n",
        "        )\n",
        "\n",
        "print(\"‚úÖ PDF Extraction functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Func»õii OCR (PaddleOCR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_image_ocr(image_array: np.ndarray, min_confidence: float = 0.5) -> Tuple[str, float]:\n",
        "    \"\"\"\n",
        "    ProceseazƒÉ imagine cu OCR »ôi returneazƒÉ (text, confidence)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ocr_result = ocr_model.ocr(image_array, cls=True)\n",
        "        \n",
        "        extracted_lines = []\n",
        "        confidences = []\n",
        "        \n",
        "        if ocr_result and ocr_result[0]:\n",
        "            for line in ocr_result[0]:\n",
        "                text = line[1][0]\n",
        "                confidence = line[1][1]\n",
        "                \n",
        "                if confidence >= min_confidence:\n",
        "                    extracted_lines.append(text)\n",
        "                    confidences.append(confidence)\n",
        "        \n",
        "        full_text = \"\\n\".join(extracted_lines)\n",
        "        avg_confidence = np.mean(confidences) if confidences else 0.0\n",
        "        \n",
        "        return full_text, avg_confidence\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"OCR failed: {e}\")\n",
        "        return \"\", 0.0\n",
        "\n",
        "def process_images_selective(images: List[ImageData], priority_threshold: float = 0.7) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    ProceseazƒÉ doar imagini cu priority >= threshold\n",
        "    \"\"\"\n",
        "    processed = []\n",
        "    \n",
        "    for img_data in images:\n",
        "        if img_data.priority_score < priority_threshold:\n",
        "            continue\n",
        "        \n",
        "        if img_data.image_array is None:\n",
        "            continue\n",
        "        \n",
        "        ocr_text, confidence = process_image_ocr(img_data.image_array)\n",
        "        \n",
        "        if ocr_text.strip():\n",
        "            processed.append({\n",
        "                'page_number': img_data.page_number,\n",
        "                'ocr_text': ocr_text,\n",
        "                'confidence': confidence\n",
        "            })\n",
        "    \n",
        "    return processed\n",
        "\n",
        "print(\"‚úÖ OCR functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Func»õii Chunking cu Deduplication\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Chunk:\n",
        "    \"\"\"Chunk de text cu metadata\"\"\"\n",
        "    text: str\n",
        "    chunk_id: str\n",
        "    chunk_hash: str\n",
        "    source_page: int\n",
        "    page_offset: int\n",
        "    metadata: Dict\n",
        "\n",
        "class TextChunker:\n",
        "    \"\"\"Smart chunker cu overlap »ôi MD5 deduplication\"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size: int = 500, overlap: int = 50, min_chunk_length: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "        self.min_chunk_length = min_chunk_length\n",
        "    \n",
        "    def chunk_text(self, text: str, source_page: int = None, remove_duplicates: bool = True) -> List[Chunk]:\n",
        "        \"\"\"Split text √Æn chunks inteligente\"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return []\n",
        "        \n",
        "        text = self._clean_text(text)\n",
        "        raw_chunks = self._split_chunks(text)\n",
        "        \n",
        "        chunk_objects = []\n",
        "        seen_hashes: Set[str] = set()\n",
        "        \n",
        "        for i, chunk_text in enumerate(raw_chunks):\n",
        "            if len(chunk_text) < self.min_chunk_length:\n",
        "                continue\n",
        "            \n",
        "            chunk_hash = hashlib.md5(chunk_text.encode()).hexdigest()\n",
        "            \n",
        "            if remove_duplicates and chunk_hash in seen_hashes:\n",
        "                continue\n",
        "            \n",
        "            seen_hashes.add(chunk_hash)\n",
        "            \n",
        "            chunk = Chunk(\n",
        "                text=chunk_text,\n",
        "                chunk_id=f\"chunk_{len(chunk_objects)}\",\n",
        "                chunk_hash=chunk_hash,\n",
        "                source_page=source_page or 0,\n",
        "                page_offset=i,\n",
        "                metadata={\n",
        "                    'char_count': len(chunk_text),\n",
        "                    'word_count': len(chunk_text.split())\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            chunk_objects.append(chunk)\n",
        "        \n",
        "        return chunk_objects\n",
        "    \n",
        "    def _split_chunks(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text cu overlap\"\"\"\n",
        "        sentences = self._split_sentences(text)\n",
        "        \n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "            \n",
        "            if len(test_chunk) > self.chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunks.append(current_chunk)\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk = test_chunk\n",
        "        \n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "        \n",
        "        if self.overlap > 0 and len(chunks) > 1:\n",
        "            chunks = self._add_overlap(chunks)\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def _add_overlap(self, chunks: List[str]) -> List[str]:\n",
        "        \"\"\"AdaugƒÉ overlap √Æntre chunks\"\"\"\n",
        "        overlapped = []\n",
        "        \n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if i == 0:\n",
        "                overlapped.append(chunk)\n",
        "            else:\n",
        "                prev_overlap = overlapped[i - 1][-self.overlap:]\n",
        "                new_chunk = prev_overlap + \" \" + chunk\n",
        "                overlapped.append(new_chunk)\n",
        "        \n",
        "        return overlapped\n",
        "    \n",
        "    def _split_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Split la punctua»õie\"\"\"\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        \n",
        "        expanded = []\n",
        "        for sent in sentences:\n",
        "            parts = sent.split('\\n')\n",
        "            expanded.extend([p.strip() for p in parts if p.strip()])\n",
        "        \n",
        "        return expanded\n",
        "    \n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"CurƒÉ»õƒÉ text\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\t')\n",
        "        return text.strip()\n",
        "\n",
        "print(\"‚úÖ Chunking functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Func»õii Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_embeddings_batch(texts: List[str], batch_size: int = 128, show_progress: bool = True) -> List[np.ndarray]:\n",
        "    \"\"\"GenereazƒÉ embeddings pentru liste de texte\"\"\"\n",
        "    if not texts:\n",
        "        return []\n",
        "    \n",
        "    all_embeddings = []\n",
        "    \n",
        "    iterator = tqdm(range(0, len(texts), batch_size), desc=\"Embeddings\") if show_progress else range(0, len(texts), batch_size)\n",
        "    \n",
        "    for i in iterator:\n",
        "        batch = texts[i:i + batch_size]\n",
        "        batch_embeddings = embedding_model.encode(batch, batch_size=len(batch), convert_to_numpy=True)\n",
        "        all_embeddings.extend(batch_embeddings)\n",
        "    \n",
        "    return all_embeddings\n",
        "\n",
        "print(\"‚úÖ Embedding functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Func»õii Supabase Upload\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_to_supabase(records: List[Dict], batch_size: int = 50, max_retries: int = 3) -> Dict:\n",
        "    \"\"\"Upload batch √Æn Supabase cu retry logic\"\"\"\n",
        "    success_count = 0\n",
        "    failed_count = 0\n",
        "    \n",
        "    batches = [records[i:i + batch_size] for i in range(0, len(records), batch_size)]\n",
        "    \n",
        "    for batch_idx, batch in enumerate(tqdm(batches, desc=\"Upload Supabase\")):\n",
        "        # Format embeddings ca string\n",
        "        formatted_batch = []\n",
        "        for item in batch:\n",
        "            embedding_str = '[' + ','.join(str(x) for x in item['embedding']) + ']'\n",
        "            \n",
        "            formatted_item = {\n",
        "                'chunk_id': item['chunk_id'],\n",
        "                'text': item['text'][:10000],\n",
        "                'embedding': embedding_str,\n",
        "                'source_pdf': item.get('source_pdf', 'unknown'),\n",
        "                'page_num': int(item.get('page_num', 0)),\n",
        "                'clasa': int(item.get('clasa', 0)) if item.get('clasa') else None,\n",
        "                'materie': item.get('materie'),\n",
        "                'capitol': item.get('capitol'),\n",
        "                'chunk_hash': item.get('chunk_hash', ''),\n",
        "                'has_images': bool(item.get('has_images', False))\n",
        "            }\n",
        "            formatted_batch.append(formatted_item)\n",
        "        \n",
        "        # Retry logic\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                supabase.table('document_embeddings').insert(formatted_batch).execute()\n",
        "                success_count += len(batch)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    failed_count += len(batch)\n",
        "                    logger.error(f\"Batch {batch_idx} failed: {e}\")\n",
        "                else:\n",
        "                    time.sleep(2)\n",
        "    \n",
        "    return {'success': success_count, 'failed': failed_count}\n",
        "\n",
        "print(\"‚úÖ Supabase upload functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Pipeline Principal - Procesare PDF Complet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_single_pdf(pdf_path: str, chunker: TextChunker, ocr_threshold: float = 0.7) -> Tuple[List[Dict], Optional[PDFExtractionResult]]:\n",
        "    \"\"\"\n",
        "    ProceseazƒÉ un PDF complet: extract ‚Üí OCR ‚Üí chunk ‚Üí embeddings\n",
        "\n",
        "    Returns: (records, extraction_result) pentru Supabase\n",
        "    \"\"\"\n",
        "    logger.info(f\"\\n{'='*60}\")\n",
        "    logger.info(f\"Procesare: {Path(pdf_path).name}\")\n",
        "    logger.info(f\"{'='*60}\")\n",
        "\n",
        "    # 1. Extrac»õie PDF\n",
        "    extraction_result = extract_text_and_images(pdf_path)\n",
        "\n",
        "    if extraction_result.extraction_status != \"success\":\n",
        "        logger.error(f\"Extrac»õie failed: {extraction_result.error_message}\")\n",
        "        return [], extraction_result\n",
        "\n",
        "    # 2. OCR pe imagini high-priority\n",
        "    ocr_texts = []\n",
        "    if extraction_result.images:\n",
        "        logger.info(f\"  üîç Procesare {len(extraction_result.images)} imagini (threshold={ocr_threshold})\")\n",
        "        ocr_results = process_images_selective(extraction_result.images, priority_threshold=ocr_threshold)\n",
        "        ocr_texts = [r['ocr_text'] for r in ocr_results]\n",
        "        logger.info(f\"  ‚úÖ OCR extras din {len(ocr_results)} imagini\")\n",
        "\n",
        "    # 3. CombinƒÉ text PDF + OCR\n",
        "    combined_text = extraction_result.text\n",
        "    if ocr_texts:\n",
        "        combined_text += \"\\n\\n\" + \"\\n\\n\".join(ocr_texts)\n",
        "\n",
        "    if not combined_text.strip():\n",
        "        logger.warning(f\"  ‚ö†Ô∏è  Niciun text extras\")\n",
        "        return [], extraction_result\n",
        "\n",
        "    # 4. Chunking\n",
        "    logger.info(f\"  ‚úÇÔ∏è  Chunking text...\")\n",
        "    chunks = chunker.chunk_text(combined_text, source_page=1, remove_duplicates=True)\n",
        "    logger.info(f\"  ‚úÖ {len(chunks)} chunks generate\")\n",
        "\n",
        "    if not chunks:\n",
        "        return [], extraction_result\n",
        "\n",
        "    # 5. Generare embeddings\n",
        "    logger.info(f\"  üßÆ Generare embeddings...\")\n",
        "    chunk_texts = [c.text for c in chunks]\n",
        "    embeddings = generate_embeddings_batch(chunk_texts, batch_size=128, show_progress=False)\n",
        "\n",
        "    # 6. Construire records pentru Supabase\n",
        "    metadata = extraction_result.metadata\n",
        "    records = []\n",
        "\n",
        "    for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "        record = {\n",
        "            'chunk_id': f\"{metadata['source_pdf']}_{chunk.chunk_hash[:8]}_{idx}\",\n",
        "            'text': chunk.text,\n",
        "            'embedding': embedding.tolist(),\n",
        "            'source_pdf': metadata['source_pdf'],\n",
        "            'page_num': chunk.source_page,\n",
        "            'clasa': metadata.get('clasa'),\n",
        "            'materie': metadata.get('materie'),\n",
        "            'capitol': None,\n",
        "            'chunk_hash': chunk.chunk_hash,\n",
        "            'has_images': len(extraction_result.images) > 0\n",
        "        }\n",
        "        records.append(record)\n",
        "\n",
        "    logger.info(f\"  ‚úÖ {len(records)} records pregƒÉtite pentru upload\")\n",
        "    return records, extraction_result\n",
        "\n",
        "print(\"‚úÖ Pipeline principal loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Checkpoint & Resume System (local + Supabase)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHECKPOINT_FILE = '/kaggle/working/processing_checkpoint.pkl'\n",
        "STATUS_TABLE = 'pdf_processing_status'\n",
        "\n",
        "def relative_pdf_path(pdf_path: str, base_path: str) -> str:\n",
        "    try:\n",
        "        return str(Path(pdf_path).resolve().relative_to(Path(base_path).resolve()))\n",
        "    except Exception:\n",
        "        return Path(pdf_path).name\n",
        "\n",
        "def normalize_checkpoint_entries(entries: List[str], base_path: str) -> List[str]:\n",
        "    normalized = []\n",
        "    for item in entries:\n",
        "        normalized.append(relative_pdf_path(item, base_path))\n",
        "    return normalized\n",
        "\n",
        "def save_checkpoint(base_path: str, processed_files: List[str], failed_files: List[str]):\n",
        "    \"\"\"SalveazƒÉ progres local (relative paths)\"\"\"\n",
        "    checkpoint = {\n",
        "        'processed': processed_files,\n",
        "        'failed': failed_files,\n",
        "        'base_path': base_path,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    with open(CHECKPOINT_FILE, 'wb') as f:\n",
        "        pickle.dump(checkpoint, f)\n",
        "\n",
        "    logger.info(f\"üíæ Checkpoint: {len(processed_files)} procesate, {len(failed_files)} failed\")\n",
        "\n",
        "def load_checkpoint(base_path: str) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"√éncarcƒÉ checkpoint »ôi normalizeazƒÉ cƒÉile\"\"\"\n",
        "    if not os.path.exists(CHECKPOINT_FILE):\n",
        "        return [], []\n",
        "\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'rb') as f:\n",
        "            checkpoint = pickle.load(f)\n",
        "        logger.info(f\"üìÇ Checkpoint gƒÉsit: {checkpoint.get('timestamp','n/a')}\")\n",
        "        processed_raw = checkpoint.get('processed', [])\n",
        "        failed_raw = checkpoint.get('failed', [])\n",
        "        return normalize_checkpoint_entries(processed_raw, base_path), normalize_checkpoint_entries(failed_raw, base_path)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"‚ö†Ô∏è  Nu pot √ÆncƒÉrca checkpoint: {e}\")\n",
        "        return [], []\n",
        "\n",
        "def upsert_pdf_status(pdf_relative_path: str, source_pdf: str, status: str, total_pages: Optional[int] = None, chunks_uploaded: Optional[int] = None, error_message: Optional[str] = None):\n",
        "    \"\"\"MarcheazƒÉ progresul √Æn Supabase (per PDF)\"\"\"\n",
        "    payload = {\n",
        "        'pdf_relative_path': pdf_relative_path,\n",
        "        'source_pdf': source_pdf,\n",
        "        'status': status,\n",
        "        'total_pages': total_pages,\n",
        "        'chunks_uploaded': chunks_uploaded,\n",
        "        'error_message': error_message,\n",
        "        'updated_at': datetime.utcnow().isoformat()\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        supabase.table(STATUS_TABLE).upsert(payload, on_conflict='pdf_relative_path').execute()\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"‚ö†Ô∏è  Nu pot scrie status √Æn Supabase: {e}\")\n",
        "\n",
        "def fetch_supabase_progress() -> Dict[str, str]:\n",
        "    \"\"\"ReturneazƒÉ status per PDF din Supabase (dict: rel_path -> status)\"\"\"\n",
        "    try:\n",
        "        response = supabase.table(STATUS_TABLE).select('pdf_relative_path,status').execute()\n",
        "        data = response.data or []\n",
        "        return {item['pdf_relative_path']: item['status'] for item in data}\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"‚ö†Ô∏è  Nu pot citi status-urile din Supabase: {e}\")\n",
        "        return {}\n",
        "\n",
        "print(\"‚úÖ Checkpoint & resume system loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Main Pipeline - Batch Processing (resume-friendly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_all_pdfs(base_path: str) -> List[str]:\n",
        "    \"\"\"GƒÉse»ôte toate PDF-urile √Æn structura de foldere\"\"\"\n",
        "    pdf_files = []\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return sorted(pdf_files)\n",
        "\n",
        "def merge_processed_sources(base_path: str, checkpoint_processed: List[str], supabase_status: Dict[str, str]) -> Set[str]:\n",
        "    completed_remote = {k for k, v in supabase_status.items() if v == 'completed'}\n",
        "    return set(checkpoint_processed) | completed_remote\n",
        "\n",
        "def main_pipeline(base_path: str, batch_upload_size: int = 50, checkpoint_interval: int = 3, max_files_per_run: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Pipeline complet de procesare PDF ‚Üí Supabase cu resume incremental\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöÄ START PIPELINE - AI Educational\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # GƒÉse»ôte PDFs\n",
        "    all_pdfs = find_all_pdfs(base_path)\n",
        "    print(f\"\\nüìö PDF-uri gƒÉsite: {len(all_pdfs)}\")\n",
        "\n",
        "    if not all_pdfs:\n",
        "        print(\"‚ùå Niciun PDF gƒÉsit!\")\n",
        "        return\n",
        "\n",
        "    # √éncarcƒÉ checkpoint + Supabase status\n",
        "    processed_files, failed_files = load_checkpoint(base_path)\n",
        "    supabase_status = fetch_supabase_progress()\n",
        "    processed_set = merge_processed_sources(base_path, processed_files, supabase_status)\n",
        "    processed_files = list(processed_set)\n",
        "\n",
        "    remaining_pdfs = [pdf for pdf in all_pdfs if relative_pdf_path(pdf, base_path) not in processed_set]\n",
        "    if max_files_per_run:\n",
        "        remaining_pdfs = remaining_pdfs[:max_files_per_run]\n",
        "\n",
        "    print(f\"‚úÖ Deja procesate: {len(processed_set)}\")\n",
        "    print(f\"‚è≥ De procesat √Æn acest run: {len(remaining_pdfs)}\")\n",
        "\n",
        "    # Ini»õializare chunker\n",
        "    chunker = TextChunker(chunk_size=500, overlap=50, min_chunk_length=50)\n",
        "\n",
        "    # Stats\n",
        "    total_chunks_uploaded = 0\n",
        "    total_errors = 0\n",
        "\n",
        "    # Procesare cu progress\n",
        "    for idx, pdf_path in enumerate(tqdm(remaining_pdfs, desc=\"üìÑ Procesare PDF-uri\")):\n",
        "        pdf_rel = relative_pdf_path(pdf_path, base_path)\n",
        "        source_pdf_name = Path(pdf_path).name\n",
        "        upsert_pdf_status(pdf_rel, source_pdf_name, status='processing')\n",
        "        try:\n",
        "            # ProceseazƒÉ PDF\n",
        "            records, extraction_result = process_single_pdf(pdf_path, chunker, ocr_threshold=0.7)\n",
        "\n",
        "            if records:\n",
        "                # Upload Supabase\n",
        "                stats = upload_to_supabase(records, batch_size=batch_upload_size)\n",
        "                total_chunks_uploaded += stats['success']\n",
        "                total_errors += stats['failed']\n",
        "                processed_files.append(pdf_rel)\n",
        "                upsert_pdf_status(\n",
        "                    pdf_rel,\n",
        "                    source_pdf_name,\n",
        "                    status='completed',\n",
        "                    total_pages=extraction_result.total_pages,\n",
        "                    chunks_uploaded=stats['success'],\n",
        "                    error_message=None\n",
        "                )\n",
        "            else:\n",
        "                failed_files.append(pdf_rel)\n",
        "                upsert_pdf_status(\n",
        "                    pdf_rel,\n",
        "                    source_pdf_name,\n",
        "                    status='failed',\n",
        "                    total_pages=extraction_result.total_pages if extraction_result else None,\n",
        "                    chunks_uploaded=0,\n",
        "                    error_message='No records generated'\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Eroare proces PDF {Path(pdf_path).name}: {e}\")\n",
        "            failed_files.append(pdf_rel)\n",
        "            upsert_pdf_status(pdf_rel, source_pdf_name, status='failed', error_message=str(e))\n",
        "\n",
        "        # Checkpoint periodic\n",
        "        if (idx + 1) % checkpoint_interval == 0:\n",
        "            save_checkpoint(base_path, processed_files, failed_files)\n",
        "\n",
        "    # Checkpoint final\n",
        "    save_checkpoint(base_path, processed_files, failed_files)\n",
        "\n",
        "    # Stats finale\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ PIPELINE COMPLET\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìÑ PDF-uri procesate (cumulative): {len(processed_files)}\")\n",
        "    print(f\"‚ùå PDF-uri failed (cumulative): {len(failed_files)}\")\n",
        "    print(f\"üì¶ Total chunks upload √Æn acest run: {total_chunks_uploaded}\")\n",
        "    print(f\"‚ö†Ô∏è  Total erori upload: {total_errors}\")\n",
        "\n",
        "    # Stats Supabase\n",
        "    try:\n",
        "        response = supabase.table('document_embeddings').select('count', count='exact').execute()\n",
        "        print(f\"\\nüìä Total vectors √Æn Supabase: {response.count:,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Nu pot accesa stats: {e}\")\n",
        "\n",
        "print(\"‚úÖ Main pipeline (incremental) loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Resume / Incremental Run (50-100 PDF-uri per sesiune)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "PDF_BASE_PATH = '/kaggle/input/pdf-files'\n",
        "BATCH_UPLOAD_SIZE = 50\n",
        "CHECKPOINT_INTERVAL = 3\n",
        "MAX_FILES_PER_RUN = 100  # proceseazƒÉ √Æn batch-uri 50-100 pentru sesiuni scurte\n",
        "RUN_INCREMENTAL = False  # seteazƒÉ True c√¢nd vrei sƒÉ rulezi procesarea incrementalƒÉ\n\n",
        "if RUN_INCREMENTAL:\n",
        "    main_pipeline(\n",
        "        base_path=PDF_BASE_PATH,\n",
        "        batch_upload_size=BATCH_UPLOAD_SIZE,\n",
        "        checkpoint_interval=CHECKPOINT_INTERVAL,\n",
        "        max_files_per_run=MAX_FILES_PER_RUN\n",
        "    )\n",
        "else:\n",
        "    print('‚ÑπÔ∏è RUN_INCREMENTAL este False - activeazƒÉ pentru a porni pipeline-ul incremental')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. TEST MODE - Procesare 1 PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurare\n",
        "PDF_BASE_PATH = '/kaggle/input/pdf-files'\n",
        "TEST_MODE = True\n",
        "\n",
        "if TEST_MODE:\n",
        "    print(\"üß™ TEST MODE - procesare 1 PDF\\n\")\n",
        "\n",
        "    all_pdfs = find_all_pdfs(PDF_BASE_PATH)\n",
        "\n",
        "    if all_pdfs:\n",
        "        test_pdf = all_pdfs[0]\n",
        "        print(f\"Test PDF: {test_pdf}\\n\")\n",
        "\n",
        "        chunker = TextChunker(chunk_size=500, overlap=50)\n",
        "        records, extraction_result = process_single_pdf(test_pdf, chunker, ocr_threshold=0.7)\n",
        "\n",
        "        if records:\n",
        "            print(f\"\\n‚úÖ {len(records)} records generate\")\n",
        "            print(f\"\\nüì¶ Exemplu record:\")\n",
        "            example = {k: v if k != 'embedding' else f\"[{len(v)} dims]\" for k, v in records[0].items()}\n",
        "            print(json.dumps(example, indent=2, ensure_ascii=False))\n",
        "\n",
        "            # Test upload 1 record\n",
        "            print(f\"\\nüß™ Test upload Supabase (primul record)...\")\n",
        "            stats = upload_to_supabase([records[0]], batch_size=1)\n",
        "\n",
        "            if stats['success'] > 0:\n",
        "                print(\"‚úÖ Upload reu»ôit\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Upload e»ôuat\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Niciun record generat pentru test\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Niciun PDF gƒÉsit pentru test\")\n",
        "else:\n",
        "    print('‚ÑπÔ∏è TEST_MODE este False - sari peste testul rapid')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}