{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Educational - PDF to Supabase Pipeline (Production-Ready)\n",
        "\n",
        "**Bazat pe modulele Python testate local de Edd**\n",
        "\n",
        "- PyMuPDF (fitz) pentru extracÈ›ie rapidÄƒ\n",
        "- PaddleOCR pentru imagini educaÈ›ionale\n",
        "- Smart chunking cu MD5 deduplication\n",
        "- paraphrase-multilingual-mpnet-base-v2 (768 dim)\n",
        "- Batch upload optimizat Supabase\n",
        "\n",
        "StructurÄƒ input: `/kaggle/input/pdf-files/{clasa}/{materie}/file.pdf`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Verificare DependenÈ›e Pre-instalate Kaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "print(f\"Python: {sys.version}\")\n",
        "\n",
        "# Check librÄƒrii critice\n",
        "dependencies_check = {\n",
        "    'fitz (PyMuPDF)': False,\n",
        "    'paddleocr': False,\n",
        "    'sentence_transformers': False,\n",
        "    'supabase': False,\n",
        "    'tqdm': False\n",
        "}\n",
        "\n",
        "try:\n",
        "    import fitz\n",
        "    dependencies_check['fitz (PyMuPDF)'] = True\n",
        "    print(f\"âœ… PyMuPDF: {fitz.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"âŒ PyMuPDF not found\")\n",
        "\n",
        "try:\n",
        "    import paddleocr\n",
        "    dependencies_check['paddleocr'] = True\n",
        "    print(f\"âœ… PaddleOCR installed\")\n",
        "except ImportError:\n",
        "    print(\"âŒ PaddleOCR not found\")\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    dependencies_check['sentence_transformers'] = True\n",
        "    print(f\"âœ… sentence-transformers installed\")\n",
        "except ImportError:\n",
        "    print(\"âŒ sentence-transformers not found\")\n",
        "\n",
        "try:\n",
        "    import supabase\n",
        "    dependencies_check['supabase'] = True\n",
        "    print(f\"âœ… supabase installed\")\n",
        "except ImportError:\n",
        "    print(\"âŒ supabase not found\")\n",
        "\n",
        "try:\n",
        "    import tqdm\n",
        "    dependencies_check['tqdm'] = True\n",
        "    print(f\"âœ… tqdm installed\")\n",
        "except ImportError:\n",
        "    print(\"âŒ tqdm not found\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Instalare DependenÈ›e LipsÄƒ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# InstaleazÄƒ DOAR ce lipseÈ™te\n",
        "!pip install -q PyMuPDF==1.23.8\n",
        "!pip install -q paddleocr==2.7.0.3\n",
        "!pip install -q sentence-transformers==2.2.2\n",
        "!pip install -q supabase==2.3.0\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"âœ… Toate dependenÈ›ele instalate\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import-uri Principale\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "import hashlib\n",
        "import time\n",
        "import pickle\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# PDF processing\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# OCR\n",
        "from paddleocr import PaddleOCR\n",
        "\n",
        "# Embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Supabase\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"âœ… Import-uri complete\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configurare Kaggle Secrets & Supabase\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "user_secrets = UserSecretsClient()\n",
        "\n",
        "SUPABASE_URL = user_secrets.get_secret(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = user_secrets.get_secret(\"SUPABASE_KEY\")\n",
        "\n",
        "# IniÈ›ializare Supabase client\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "print(\"âœ… Supabase conectat\")\n",
        "\n",
        "# Test conexiune\n",
        "try:\n",
        "    response = supabase.table('document_embeddings').select('*').limit(1).execute()\n",
        "    print(f\"âœ… Test query OK\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Test query failed: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configurare Modele (Embeddings + OCR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === EMBEDDING MODEL ===\n",
        "MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'\n",
        "EMBEDDING_DIM = 768\n",
        "\n",
        "print(f\"ðŸ“¥ ÃŽncÄƒrcare embedding model: {MODEL_NAME}\")\n",
        "embedding_model = SentenceTransformer(MODEL_NAME, device='cpu')\n",
        "\n",
        "# Test\n",
        "test_emb = embedding_model.encode([\"Test text\"])\n",
        "assert len(test_emb[0]) == EMBEDDING_DIM\n",
        "print(f\"âœ… Embedding model loaded: {EMBEDDING_DIM} dimensiuni\")\n",
        "\n",
        "# === OCR MODEL ===\n",
        "print(f\"ðŸ“¥ ÃŽncÄƒrcare PaddleOCR (ro + en)...\")\n",
        "ocr_model = PaddleOCR(\n",
        "    use_angle_cls=True,\n",
        "    lang=['ro', 'en'],\n",
        "    use_gpu=False,  # CPU mai stabil pe Kaggle\n",
        "    show_log=False\n",
        ")\n",
        "print(f\"âœ… PaddleOCR loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Clase & FuncÈ›ii - PDF Extraction (PyMuPDF)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ImageData:\n",
        "    \"\"\"InformaÈ›ii despre o imagine din PDF\"\"\"\n",
        "    page_number: int\n",
        "    bbox: Tuple[float, float, float, float]\n",
        "    size_bytes: int\n",
        "    priority_score: float\n",
        "    image_type: str\n",
        "    image_array: Optional[np.ndarray] = None  # Pentru OCR\n",
        "\n",
        "@dataclass\n",
        "class PDFExtractionResult:\n",
        "    \"\"\"Rezultat extracÈ›ie PDF\"\"\"\n",
        "    pdf_path: str\n",
        "    text: str\n",
        "    images: List[ImageData]\n",
        "    total_pages: int\n",
        "    file_size_bytes: int\n",
        "    metadata: Dict\n",
        "    extraction_status: str\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "def extract_metadata_from_path(pdf_path: str) -> Dict:\n",
        "    \"\"\"Extrage clasa È™i materia din structura de foldere\"\"\"\n",
        "    path_parts = Path(pdf_path).parts\n",
        "    \n",
        "    metadata = {\n",
        "        'clasa': None,\n",
        "        'materie': None,\n",
        "        'source_pdf': Path(pdf_path).name\n",
        "    }\n",
        "    \n",
        "    # CÄƒutare clasa\n",
        "    for part in path_parts:\n",
        "        if 'clasa' in part.lower() or 'class' in part.lower():\n",
        "            match = re.search(r'\\d+', part)\n",
        "            if match:\n",
        "                metadata['clasa'] = int(match.group())\n",
        "                break\n",
        "    \n",
        "    # CÄƒutare materie\n",
        "    if len(path_parts) >= 2:\n",
        "        metadata['materie'] = path_parts[-2]\n",
        "    \n",
        "    return metadata\n",
        "\n",
        "def calculate_image_priority(pix: fitz.Pixmap, image_size: int) -> float:\n",
        "    \"\"\"Priority score 0.0-1.0 pentru OCR (imagini cu text = prioritate Ã®naltÄƒ)\"\"\"\n",
        "    score = 0.5\n",
        "    \n",
        "    try:\n",
        "        size_score = min(image_size / 512000, 1.0)\n",
        "        \n",
        "        width = pix.width\n",
        "        height = pix.height\n",
        "        aspect_ratio = max(width, height) / max(min(width, height), 1)\n",
        "        \n",
        "        if 0.8 < aspect_ratio < 1.25:\n",
        "            ratio_score = 0.9  # Aproape pÄƒtrate - diagrame\n",
        "        else:\n",
        "            ratio_score = 0.5\n",
        "        \n",
        "        score = (size_score * 0.4) + (ratio_score * 0.6)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return score\n",
        "\n",
        "def detect_image_type(pix: fitz.Pixmap) -> str:\n",
        "    \"\"\"DetecteazÄƒ tipul imaginii\"\"\"\n",
        "    try:\n",
        "        width = pix.width\n",
        "        height = pix.height\n",
        "        aspect_ratio = max(width, height) / max(min(width, height), 1)\n",
        "        \n",
        "        if width > 1000 or height > 1000:\n",
        "            return \"chart\"\n",
        "        elif width < 200 or height < 200:\n",
        "            return \"diagram\"\n",
        "        elif 0.8 < aspect_ratio < 1.25:\n",
        "            return \"diagram\"\n",
        "        else:\n",
        "            return \"photo\"\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def extract_text_and_images(pdf_path: str) -> PDFExtractionResult:\n",
        "    \"\"\"Extrage text È™i imagini din PDF cu PyMuPDF\"\"\"\n",
        "    try:\n",
        "        pdf_path = str(pdf_path)\n",
        "        \n",
        "        if not Path(pdf_path).exists():\n",
        "            return PDFExtractionResult(\n",
        "                pdf_path=pdf_path, text=\"\", images=[], total_pages=0,\n",
        "                file_size_bytes=0, metadata={}, extraction_status=\"error\",\n",
        "                error_message=f\"File not found: {pdf_path}\"\n",
        "            )\n",
        "        \n",
        "        pdf_document = fitz.open(pdf_path)\n",
        "        file_size_bytes = Path(pdf_path).stat().st_size\n",
        "        total_pages = len(pdf_document)\n",
        "        \n",
        "        logger.info(f\"Procesare: {Path(pdf_path).name} ({total_pages} pagini)\")\n",
        "        \n",
        "        all_text = []\n",
        "        images_found = []\n",
        "        \n",
        "        for page_number in range(total_pages):\n",
        "            page = pdf_document[page_number]\n",
        "            \n",
        "            # Extrage text\n",
        "            page_text = page.get_text()\n",
        "            all_text.append(page_text)\n",
        "            \n",
        "            # DetecteazÄƒ imagini\n",
        "            image_list = page.get_images()\n",
        "            \n",
        "            for img_index, img_info in enumerate(image_list):\n",
        "                try:\n",
        "                    xref = img_info[0]\n",
        "                    pix = fitz.Pixmap(pdf_document, xref)\n",
        "                    \n",
        "                    image_bytes = pix.tobytes()\n",
        "                    image_size = len(image_bytes)\n",
        "                    \n",
        "                    # FiltreazÄƒ imagini mici (<50KB)\n",
        "                    if image_size < 51200:\n",
        "                        continue\n",
        "                    \n",
        "                    # Bounding box\n",
        "                    image_rect = page.get_image_rects(img_info)\n",
        "                    bbox = image_rect[0] if image_rect else (0, 0, 100, 100)\n",
        "                    \n",
        "                    priority = calculate_image_priority(pix, image_size)\n",
        "                    \n",
        "                    # ConverteÈ™te pixmap la numpy array pentru OCR\n",
        "                    img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)\n",
        "                    \n",
        "                    img_data = ImageData(\n",
        "                        page_number=page_number + 1,\n",
        "                        bbox=bbox,\n",
        "                        size_bytes=image_size,\n",
        "                        priority_score=priority,\n",
        "                        image_type=detect_image_type(pix),\n",
        "                        image_array=img_array\n",
        "                    )\n",
        "                    \n",
        "                    images_found.append(img_data)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Skip image {img_index} p{page_number+1}: {e}\")\n",
        "                    continue\n",
        "        \n",
        "        combined_text = \"\\n\".join(all_text)\n",
        "        pdf_document.close()\n",
        "        \n",
        "        metadata = extract_metadata_from_path(pdf_path)\n",
        "        \n",
        "        logger.info(f\"âœ… Extras: {len(combined_text)} chars, {len(images_found)} imagini\")\n",
        "        \n",
        "        return PDFExtractionResult(\n",
        "            pdf_path=pdf_path,\n",
        "            text=combined_text,\n",
        "            images=images_found,\n",
        "            total_pages=total_pages,\n",
        "            file_size_bytes=file_size_bytes,\n",
        "            metadata=metadata,\n",
        "            extraction_status=\"success\"\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Eroare procesare {pdf_path}: {e}\")\n",
        "        return PDFExtractionResult(\n",
        "            pdf_path=pdf_path, text=\"\", images=[], total_pages=0,\n",
        "            file_size_bytes=0, metadata={}, extraction_status=\"error\",\n",
        "            error_message=str(e)\n",
        "        )\n",
        "\n",
        "print(\"âœ… PDF Extraction functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. FuncÈ›ii OCR (PaddleOCR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_image_ocr(image_array: np.ndarray, min_confidence: float = 0.5) -> Tuple[str, float]:\n",
        "    \"\"\"\n",
        "    ProceseazÄƒ imagine cu OCR È™i returneazÄƒ (text, confidence)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ocr_result = ocr_model.ocr(image_array, cls=True)\n",
        "        \n",
        "        extracted_lines = []\n",
        "        confidences = []\n",
        "        \n",
        "        if ocr_result and ocr_result[0]:\n",
        "            for line in ocr_result[0]:\n",
        "                text = line[1][0]\n",
        "                confidence = line[1][1]\n",
        "                \n",
        "                if confidence >= min_confidence:\n",
        "                    extracted_lines.append(text)\n",
        "                    confidences.append(confidence)\n",
        "        \n",
        "        full_text = \"\\n\".join(extracted_lines)\n",
        "        avg_confidence = np.mean(confidences) if confidences else 0.0\n",
        "        \n",
        "        return full_text, avg_confidence\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"OCR failed: {e}\")\n",
        "        return \"\", 0.0\n",
        "\n",
        "def process_images_selective(images: List[ImageData], priority_threshold: float = 0.7) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    ProceseazÄƒ doar imagini cu priority >= threshold\n",
        "    \"\"\"\n",
        "    processed = []\n",
        "    \n",
        "    for img_data in images:\n",
        "        if img_data.priority_score < priority_threshold:\n",
        "            continue\n",
        "        \n",
        "        if img_data.image_array is None:\n",
        "            continue\n",
        "        \n",
        "        ocr_text, confidence = process_image_ocr(img_data.image_array)\n",
        "        \n",
        "        if ocr_text.strip():\n",
        "            processed.append({\n",
        "                'page_number': img_data.page_number,\n",
        "                'ocr_text': ocr_text,\n",
        "                'confidence': confidence\n",
        "            })\n",
        "    \n",
        "    return processed\n",
        "\n",
        "print(\"âœ… OCR functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. FuncÈ›ii Chunking cu Deduplication\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Chunk:\n",
        "    \"\"\"Chunk de text cu metadata\"\"\"\n",
        "    text: str\n",
        "    chunk_id: str\n",
        "    chunk_hash: str\n",
        "    source_page: int\n",
        "    page_offset: int\n",
        "    metadata: Dict\n",
        "\n",
        "class TextChunker:\n",
        "    \"\"\"Smart chunker cu overlap È™i MD5 deduplication\"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size: int = 500, overlap: int = 50, min_chunk_length: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "        self.min_chunk_length = min_chunk_length\n",
        "    \n",
        "    def chunk_text(self, text: str, source_page: int = None, remove_duplicates: bool = True) -> List[Chunk]:\n",
        "        \"\"\"Split text Ã®n chunks inteligente\"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return []\n",
        "        \n",
        "        text = self._clean_text(text)\n",
        "        raw_chunks = self._split_chunks(text)\n",
        "        \n",
        "        chunk_objects = []\n",
        "        seen_hashes: Set[str] = set()\n",
        "        \n",
        "        for i, chunk_text in enumerate(raw_chunks):\n",
        "            if len(chunk_text) < self.min_chunk_length:\n",
        "                continue\n",
        "            \n",
        "            chunk_hash = hashlib.md5(chunk_text.encode()).hexdigest()\n",
        "            \n",
        "            if remove_duplicates and chunk_hash in seen_hashes:\n",
        "                continue\n",
        "            \n",
        "            seen_hashes.add(chunk_hash)\n",
        "            \n",
        "            chunk = Chunk(\n",
        "                text=chunk_text,\n",
        "                chunk_id=f\"chunk_{len(chunk_objects)}\",\n",
        "                chunk_hash=chunk_hash,\n",
        "                source_page=source_page or 0,\n",
        "                page_offset=i,\n",
        "                metadata={\n",
        "                    'char_count': len(chunk_text),\n",
        "                    'word_count': len(chunk_text.split())\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            chunk_objects.append(chunk)\n",
        "        \n",
        "        return chunk_objects\n",
        "    \n",
        "    def _split_chunks(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text cu overlap\"\"\"\n",
        "        sentences = self._split_sentences(text)\n",
        "        \n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "            \n",
        "            if len(test_chunk) > self.chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunks.append(current_chunk)\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk = test_chunk\n",
        "        \n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "        \n",
        "        if self.overlap > 0 and len(chunks) > 1:\n",
        "            chunks = self._add_overlap(chunks)\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def _add_overlap(self, chunks: List[str]) -> List[str]:\n",
        "        \"\"\"AdaugÄƒ overlap Ã®ntre chunks\"\"\"\n",
        "        overlapped = []\n",
        "        \n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if i == 0:\n",
        "                overlapped.append(chunk)\n",
        "            else:\n",
        "                prev_overlap = overlapped[i - 1][-self.overlap:]\n",
        "                new_chunk = prev_overlap + \" \" + chunk\n",
        "                overlapped.append(new_chunk)\n",
        "        \n",
        "        return overlapped\n",
        "    \n",
        "    def _split_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Split la punctuaÈ›ie\"\"\"\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        \n",
        "        expanded = []\n",
        "        for sent in sentences:\n",
        "            parts = sent.split('\\n')\n",
        "            expanded.extend([p.strip() for p in parts if p.strip()])\n",
        "        \n",
        "        return expanded\n",
        "    \n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"CurÄƒÈ›Äƒ text\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\t')\n",
        "        return text.strip()\n",
        "\n",
        "print(\"âœ… Chunking functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. FuncÈ›ii Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_embeddings_batch(texts: List[str], batch_size: int = 128, show_progress: bool = True) -> List[np.ndarray]:\n",
        "    \"\"\"GenereazÄƒ embeddings pentru liste de texte\"\"\"\n",
        "    if not texts:\n",
        "        return []\n",
        "    \n",
        "    all_embeddings = []\n",
        "    \n",
        "    iterator = tqdm(range(0, len(texts), batch_size), desc=\"Embeddings\") if show_progress else range(0, len(texts), batch_size)\n",
        "    \n",
        "    for i in iterator:\n",
        "        batch = texts[i:i + batch_size]\n",
        "        batch_embeddings = embedding_model.encode(batch, batch_size=len(batch), convert_to_numpy=True)\n",
        "        all_embeddings.extend(batch_embeddings)\n",
        "    \n",
        "    return all_embeddings\n",
        "\n",
        "print(\"âœ… Embedding functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. FuncÈ›ii Supabase Upload\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_to_supabase(records: List[Dict], batch_size: int = 50, max_retries: int = 3) -> Dict:\n",
        "    \"\"\"Upload batch Ã®n Supabase cu retry logic\"\"\"\n",
        "    success_count = 0\n",
        "    failed_count = 0\n",
        "    \n",
        "    batches = [records[i:i + batch_size] for i in range(0, len(records), batch_size)]\n",
        "    \n",
        "    for batch_idx, batch in enumerate(tqdm(batches, desc=\"Upload Supabase\")):\n",
        "        # Format embeddings ca string\n",
        "        formatted_batch = []\n",
        "        for item in batch:\n",
        "            embedding_str = '[' + ','.join(str(x) for x in item['embedding']) + ']'\n",
        "            \n",
        "            formatted_item = {\n",
        "                'chunk_id': item['chunk_id'],\n",
        "                'text': item['text'][:10000],\n",
        "                'embedding': embedding_str,\n",
        "                'source_pdf': item.get('source_pdf', 'unknown'),\n",
        "                'page_num': int(item.get('page_num', 0)),\n",
        "                'clasa': int(item.get('clasa', 0)) if item.get('clasa') else None,\n",
        "                'materie': item.get('materie'),\n",
        "                'capitol': item.get('capitol'),\n",
        "                'chunk_hash': item.get('chunk_hash', ''),\n",
        "                'has_images': bool(item.get('has_images', False))\n",
        "            }\n",
        "            formatted_batch.append(formatted_item)\n",
        "        \n",
        "        # Retry logic\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                supabase.table('document_embeddings').insert(formatted_batch).execute()\n",
        "                success_count += len(batch)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    failed_count += len(batch)\n",
        "                    logger.error(f\"Batch {batch_idx} failed: {e}\")\n",
        "                else:\n",
        "                    time.sleep(2)\n",
        "    \n",
        "    return {'success': success_count, 'failed': failed_count}\n",
        "\n",
        "print(\"âœ… Supabase upload functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Pipeline Principal - Procesare PDF Complet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_single_pdf(pdf_path: str, chunker: TextChunker, ocr_threshold: float = 0.7) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    ProceseazÄƒ un PDF complet: extract â†’ OCR â†’ chunk â†’ embeddings\n",
        "    \n",
        "    Returns: Lista de records gata pentru Supabase\n",
        "    \"\"\"\n",
        "    logger.info(f\"\\n{'='*60}\")\n",
        "    logger.info(f\"Procesare: {Path(pdf_path).name}\")\n",
        "    logger.info(f\"{'='*60}\")\n",
        "    \n",
        "    # 1. ExtracÈ›ie PDF\n",
        "    extraction_result = extract_text_and_images(pdf_path)\n",
        "    \n",
        "    if extraction_result.extraction_status != \"success\":\n",
        "        logger.error(f\"ExtracÈ›ie failed: {extraction_result.error_message}\")\n",
        "        return []\n",
        "    \n",
        "    # 2. OCR pe imagini high-priority\n",
        "    ocr_texts = []\n",
        "    if extraction_result.images:\n",
        "        logger.info(f\"  ðŸ” Procesare {len(extraction_result.images)} imagini (threshold={ocr_threshold})\")\n",
        "        ocr_results = process_images_selective(extraction_result.images, priority_threshold=ocr_threshold)\n",
        "        ocr_texts = [r['ocr_text'] for r in ocr_results]\n",
        "        logger.info(f\"  âœ… OCR extras din {len(ocr_results)} imagini\")\n",
        "    \n",
        "    # 3. CombinÄƒ text PDF + OCR\n",
        "    combined_text = extraction_result.text\n",
        "    if ocr_texts:\n",
        "        combined_text += \"\\n\\n\" + \"\\n\\n\".join(ocr_texts)\n",
        "    \n",
        "    if not combined_text.strip():\n",
        "        logger.warning(f\"  âš ï¸  Niciun text extras\")\n",
        "        return []\n",
        "    \n",
        "    # 4. Chunking\n",
        "    logger.info(f\"  âœ‚ï¸  Chunking text...\")\n",
        "    chunks = chunker.chunk_text(combined_text, source_page=1, remove_duplicates=True)\n",
        "    logger.info(f\"  âœ… {len(chunks)} chunks generate\")\n",
        "    \n",
        "    if not chunks:\n",
        "        return []\n",
        "    \n",
        "    # 5. Generare embeddings\n",
        "    logger.info(f\"  ðŸ§® Generare embeddings...\")\n",
        "    chunk_texts = [c.text for c in chunks]\n",
        "    embeddings = generate_embeddings_batch(chunk_texts, batch_size=128, show_progress=False)\n",
        "    \n",
        "    # 6. Construire records pentru Supabase\n",
        "    metadata = extraction_result.metadata\n",
        "    records = []\n",
        "    \n",
        "    for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "        record = {\n",
        "            'chunk_id': f\"{metadata['source_pdf']}_{chunk.chunk_hash[:8]}_{idx}\",\n",
        "            'text': chunk.text,\n",
        "            'embedding': embedding.tolist(),\n",
        "            'source_pdf': metadata['source_pdf'],\n",
        "            'page_num': chunk.source_page,\n",
        "            'clasa': metadata.get('clasa'),\n",
        "            'materie': metadata.get('materie'),\n",
        "            'capitol': None,\n",
        "            'chunk_hash': chunk.chunk_hash,\n",
        "            'has_images': len(extraction_result.images) > 0\n",
        "        }\n",
        "        records.append(record)\n",
        "    \n",
        "    logger.info(f\"  âœ… {len(records)} records pregÄƒtite pentru upload\")\n",
        "    return records\n",
        "\n",
        "print(\"âœ… Pipeline principal loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Checkpoint System\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHECKPOINT_FILE = '/kaggle/working/processing_checkpoint.pkl'\n",
        "\n",
        "def save_checkpoint(processed_files: List[str], failed_files: List[str]):\n",
        "    \"\"\"SalveazÄƒ progres\"\"\"\n",
        "    checkpoint = {\n",
        "        'processed': processed_files,\n",
        "        'failed': failed_files,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    with open(CHECKPOINT_FILE, 'wb') as f:\n",
        "        pickle.dump(checkpoint, f)\n",
        "    \n",
        "    logger.info(f\"ðŸ’¾ Checkpoint: {len(processed_files)} procesate, {len(failed_files)} failed\")\n",
        "\n",
        "def load_checkpoint() -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"ÃŽncarcÄƒ checkpoint\"\"\"\n",
        "    if not os.path.exists(CHECKPOINT_FILE):\n",
        "        return [], []\n",
        "    \n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'rb') as f:\n",
        "            checkpoint = pickle.load(f)\n",
        "        logger.info(f\"ðŸ“‚ Checkpoint gÄƒsit: {checkpoint['timestamp']}\")\n",
        "        return checkpoint['processed'], checkpoint['failed']\n",
        "    except:\n",
        "        return [], []\n",
        "\n",
        "print(\"âœ… Checkpoint system loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Main Pipeline - Batch Processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_all_pdfs(base_path: str) -> List[str]:\n",
        "    \"\"\"GÄƒseÈ™te toate PDF-urile Ã®n structura de foldere\"\"\"\n",
        "    pdf_files = []\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return sorted(pdf_files)\n",
        "\n",
        "def main_pipeline(base_path: str, batch_upload_size: int = 50, checkpoint_interval: int = 3):\n",
        "    \"\"\"\n",
        "    Pipeline complet de procesare PDF â†’ Supabase\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ðŸš€ START PIPELINE - AI Educational\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # GÄƒseÈ™te PDFs\n",
        "    all_pdfs = find_all_pdfs(base_path)\n",
        "    print(f\"\\nðŸ“š PDF-uri gÄƒsite: {len(all_pdfs)}\")\n",
        "    \n",
        "    if not all_pdfs:\n",
        "        print(\"âŒ Niciun PDF gÄƒsit!\")\n",
        "        return\n",
        "    \n",
        "    # ÃŽncarcÄƒ checkpoint\n",
        "    processed_files, failed_files = load_checkpoint()\n",
        "    remaining_pdfs = [pdf for pdf in all_pdfs if pdf not in processed_files]\n",
        "    \n",
        "    print(f\"âœ… Deja procesate: {len(processed_files)}\")\n",
        "    print(f\"â³ De procesat: {len(remaining_pdfs)}\")\n",
        "    \n",
        "    # IniÈ›ializare chunker\n",
        "    chunker = TextChunker(chunk_size=500, overlap=50, min_chunk_length=50)\n",
        "    \n",
        "    # Stats\n",
        "    total_chunks_uploaded = 0\n",
        "    total_errors = 0\n",
        "    \n",
        "    # Procesare cu progress\n",
        "    for idx, pdf_path in enumerate(tqdm(remaining_pdfs, desc=\"ðŸ“„ Procesare PDF-uri\")):\n",
        "        try:\n",
        "            # ProceseazÄƒ PDF\n",
        "            records = process_single_pdf(pdf_path, chunker, ocr_threshold=0.7)\n",
        "            \n",
        "            if records:\n",
        "                # Upload Supabase\n",
        "                stats = upload_to_supabase(records, batch_size=batch_upload_size)\n",
        "                total_chunks_uploaded += stats['success']\n",
        "                total_errors += stats['failed']\n",
        "                \n",
        "                processed_files.append(pdf_path)\n",
        "            else:\n",
        "                failed_files.append(pdf_path)\n",
        "        \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Eroare proces PDF {Path(pdf_path).name}: {e}\")\n",
        "            failed_files.append(pdf_path)\n",
        "        \n",
        "        # Checkpoint periodic\n",
        "        if (idx + 1) % checkpoint_interval == 0:\n",
        "            save_checkpoint(processed_files, failed_files)\n",
        "    \n",
        "    # Checkpoint final\n",
        "    save_checkpoint(processed_files, failed_files)\n",
        "    \n",
        "    # Stats finale\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… PIPELINE COMPLET\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸ“„ PDF-uri procesate: {len(processed_files)}\")\n",
        "    print(f\"âŒ PDF-uri failed: {len(failed_files)}\")\n",
        "    print(f\"ðŸ“¦ Total chunks uploaded: {total_chunks_uploaded}\")\n",
        "    print(f\"âš ï¸  Total erori upload: {total_errors}\")\n",
        "    \n",
        "    # Stats Supabase\n",
        "    try:\n",
        "        response = supabase.table('document_embeddings').select('count', count='exact').execute()\n",
        "        print(f\"\\nðŸ“Š Total vectors Ã®n Supabase: {response.count:,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Nu pot accesa stats: {e}\")\n",
        "\n",
        "print(\"âœ… Main pipeline loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. TEST MODE - Procesare 1 PDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurare\n",
        "PDF_BASE_PATH = '/kaggle/input/pdf-files'\n",
        "TEST_MODE = True\n",
        "\n",
        "if TEST_MODE:\n",
        "    print(\"ðŸ§ª TEST MODE - procesare 1 PDF\\n\")\n",
        "    \n",
        "    all_pdfs = find_all_pdfs(PDF_BASE_PATH)\n",
        "    \n",
        "    if all_pdfs:\n",
        "        test_pdf = all_pdfs[0]\n",
        "        print(f\"Test PDF: {test_pdf}\\n\")\n",
        "        \n",
        "        chunker = TextChunker(chunk_size=500, overlap=50)\n",
        "        records = process_single_pdf(test_pdf, chunker, ocr_threshold=0.7)\n",
        "        \n",
        "        if records:\n",
        "            print(f\"\\nâœ… {len(records)} records generate\")\n",
        "            print(f\"\\nðŸ“¦ Exemplu record:\")\n",
        "            example = {k: v if k != 'embedding' else f\"[{len(v)} dims]\" for k, v in records[0].items()}\n",
        "            print(json.dumps(example, indent=2, ensure_ascii=False))\n",
        "            \n",
        "            # Test upload 1 record\n",
        "            print(f\"\\nðŸ§ª Test upload Supabase (primul record)...\")\n",
        "            stats = upload_to_supabase([records[0]], batch_size=1)\n",
        "            \n",
        "            if stats['success'] >\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}