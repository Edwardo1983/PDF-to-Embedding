# ğŸ—ï¸ ARCHITECTURE.md - ExplicaÈ›ie TehnicÄƒ Complete

DocumentaÈ›ie detaliatÄƒ a arhitecturii pipeline-ului PDF â†’ Embeddings â†’ Supabase.

---

## ğŸ“Š Flow Diagram Complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INPUT: 15GB PDF-uri LOCAL                    â”‚
â”‚        [C:\materiale_didactice\clasa_X\materie\*.pdf]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”œâ”€â”€â”€ [MANUAL UPLOAD TO KAGGLE]
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             KAGGLE DATASET (mounted as /kaggle/input)           â”‚
â”‚  - Full 15GB folder structure preserved                         â”‚
â”‚  - ~3,000-5,000 PDF files                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”œâ”€â”€â”€ [KAGGLE NOTEBOOK PROCESSING]
                     â”‚
                     â†“
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘  PROCESSING PIPELINE (Kaggle P100)    â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â”‚                                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚1. PDF EXTRACTION (PyMuPDF)       â”‚ â”‚
        â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
        â”‚  â”‚ Input:  PDF file                 â”‚ â”‚
        â”‚  â”‚ Extract:                         â”‚ â”‚
        â”‚  â”‚  - Text from all pages           â”‚ â”‚
        â”‚  â”‚  - Images (>50KB)                â”‚ â”‚
        â”‚  â”‚  - Page boundaries               â”‚ â”‚
        â”‚  â”‚ Output: {text, images[], meta}   â”‚ â”‚
        â”‚  â”‚ Speed: ~500 PDFs/hour (GPU acc)  â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚           â”‚                            â”‚
        â”‚           â”œâ”€ Text (~80%)               â”‚
        â”‚           â””â”€ Images (~20%)              â”‚
        â”‚                                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚2. OCR PROCESSING (PaddleOCR)     â”‚ â”‚
        â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
        â”‚  â”‚ Input:  Images from PDFs         â”‚ â”‚
        â”‚  â”‚ Process:                         â”‚ â”‚
        â”‚  â”‚  - Score images (priority)       â”‚ â”‚
        â”‚  â”‚  - Filter low-priority ones      â”‚ â”‚
        â”‚  â”‚  - Run OCR Ğ½Ğ° selected images    â”‚ â”‚
        â”‚  â”‚ Output: Extracted text + conf    â”‚ â”‚
        â”‚  â”‚ Speed: ~1,000 images/hour        â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚           â”‚                            â”‚
        â”‚           â””â”€ OCR text merged with PDF  â”‚
        â”‚                                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚3. TEXT CHUNKING & DEDUP          â”‚ â”‚
        â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
        â”‚  â”‚ Input:  Full text from PDF       â”‚ â”‚
        â”‚  â”‚ Process:                         â”‚ â”‚
        â”‚  â”‚  - Split at sentence boundaries  â”‚ â”‚
        â”‚  â”‚  - Target: ~500 chars/chunk      â”‚ â”‚
        â”‚  â”‚  - Add 50-char overlap           â”‚ â”‚
        â”‚  â”‚  - Calculate MD5 hashes          â”‚ â”‚
        â”‚  â”‚  - Remove duplicates (100 chars) â”‚ â”‚
        â”‚  â”‚ Output: [Chunk objects]          â”‚ â”‚
        â”‚  â”‚ Example:                         â”‚ â”‚
        â”‚  â”‚  Chunk1: "Aria pÄƒtrat = ... "    â”‚ â”‚
        â”‚  â”‚  Chunk2: "... = laturaÂ² "        â”‚ â”‚
        â”‚  â”‚  (Overlap: last 50 chars chk1)   â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚           â”‚                            â”‚
        â”‚           â””â”€ Dedup rate: ~2-5%         â”‚
        â”‚                                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚4. EMBEDDING GENERATION           â”‚ â”‚
        â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
        â”‚  â”‚ Model:                           â”‚ â”‚
        â”‚  â”‚  paraphrase-multilingual-        â”‚ â”‚
        â”‚  â”‚  mpnet-base-v2                   â”‚ â”‚
        â”‚  â”‚                                  â”‚ â”‚
        â”‚  â”‚ Input:  Chunks [str]             â”‚ â”‚
        â”‚  â”‚ Process:                         â”‚ â”‚
        â”‚  â”‚  - Batch size: 128 texts        â”‚ â”‚
        â”‚  â”‚  - Tokenize (max 384 tokens)     â”‚ â”‚
        â”‚  â”‚  - Forward pass (GPU)            â”‚ â”‚
        â”‚  â”‚  - Extract pooled output         â”‚ â”‚
        â”‚  â”‚  - Output: 768-dim vector        â”‚ â”‚
        â”‚  â”‚ Output: numpy arrays (N, 768)    â”‚ â”‚
        â”‚  â”‚ Speed: ~50,000 vecs/hour (GPU)   â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚           â”‚                            â”‚
        â”‚           â””â”€ One vector = semantic     â”‚
        â”‚             representation            â”‚
        â”‚                                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚5. BATCH UPLOAD TO SUPABASE       â”‚ â”‚
        â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
        â”‚  â”‚ Input:  Vectors + metadata       â”‚ â”‚
        â”‚  â”‚ Process:                         â”‚ â”‚
        â”‚  â”‚  - Group: 10,000 vectors/batch   â”‚ â”‚
        â”‚  â”‚  - Format for pgvector           â”‚ â”‚
        â”‚  â”‚  - Prepare metadata JSON         â”‚ â”‚
        â”‚  â”‚  - POST to Supabase REST API     â”‚ â”‚
        â”‚  â”‚  - Retry logic (3 attempts)      â”‚ â”‚
        â”‚  â”‚  - Wait between batches          â”‚ â”‚
        â”‚  â”‚ Output: Vectors in DB            â”‚ â”‚
        â”‚  â”‚ Speed: ~10,000/min (batched)     â”‚ â”‚
        â”‚  â”‚ Total time: 2-3 hours (600k)     â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚                                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚6. INDEX CREATION (POST-PROCESS)  â”‚ â”‚
        â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
        â”‚  â”‚ After all vectors uploaded:      â”‚ â”‚
        â”‚  â”‚  - CREATE INDEX HNSW             â”‚ â”‚
        â”‚  â”‚  - ON embedding VECTOR column    â”‚ â”‚
        â”‚  â”‚  - With cosine distance          â”‚ â”‚
        â”‚  â”‚ Time: 30-60 min (600k vectors)   â”‚ â”‚
        â”‚  â”‚ Result: ~50-100ms query latency  â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚                                        â”‚
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SUPABASE pgvector (500MB Free Tier)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Table: document_embeddings                              â”‚   â”‚
â”‚  â”‚  â”œâ”€ chunk_id: TEXT (unique)                            â”‚   â”‚
â”‚  â”‚  â”œâ”€ text: TEXT (10,000 chars max)                      â”‚   â”‚
â”‚  â”‚  â”œâ”€ embedding: VECTOR(768) â† Main column               â”‚   â”‚
â”‚  â”‚  â”œâ”€ source_pdf: TEXT (path metadata)                   â”‚   â”‚
â”‚  â”‚  â”œâ”€ page_num: INT (page in PDF)                        â”‚   â”‚
â”‚  â”‚  â”œâ”€ clasa: INT (class 0-4)                             â”‚   â”‚
â”‚  â”‚  â”œâ”€ materie: TEXT (subject)                            â”‚   â”‚
â”‚  â”‚  â”œâ”€ capitol: TEXT (chapter)                            â”‚   â”‚
â”‚  â”‚  â”œâ”€ chunk_hash: TEXT (MD5, for dedup)                  â”‚   â”‚
â”‚  â”‚  â””â”€ created_at: TIMESTAMP                              â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚ Indexes:                                               â”‚   â”‚
â”‚  â”‚  - HNSW on embedding (cosine similarity)              â”‚   â”‚
â”‚  â”‚  - Standard on clasa, materie, source_pdf             â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚ Functions (RPC):                                       â”‚   â”‚
â”‚  â”‚  - match_documents(query, match_count, filters)       â”‚   â”‚
â”‚  â”‚  - get_statistics()                                    â”‚   â”‚
â”‚  â”‚  - count_vectors()                                     â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚ Stats:                                                 â”‚   â”‚
â”‚  â”‚  - Total vectors: 400k-600k                           â”‚   â”‚
â”‚  â”‚  - Database size: ~300-500 MB                         â”‚   â”‚
â”‚  â”‚  - Vector storage: 768 floats Ã— N Ã— 4 bytes = ~2MB/k  â”‚   â”‚
â”‚  â”‚  - Index size: ~100-150 MB (HNSW overhead)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”œâ”€â”€â”€ [READY FOR AI APPLICATION]
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         YOUR AI TUTORING SYSTEM (Separate Repository)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚1. User query: "Cum se calculeazÄƒ aria unui pÄƒtrat?"    â”‚   â”‚
â”‚  â”‚2. Generate embedding cu same model                     â”‚   â”‚
â”‚  â”‚3. Query Supabase: match_documents(query_emb, top=10)   â”‚   â”‚
â”‚  â”‚4. Retrieve top-10 similar chunks + metadata            â”‚   â”‚
â”‚  â”‚5. Format as context pentru LLM (GPT/Claude)            â”‚   â”‚
â”‚  â”‚6. LLM generates tutoring response                      â”‚   â”‚
â”‚  â”‚7. User gets personalized answer!                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Detailed Component Explanation

### 1. **PDF EXTRACTION (scripts/pdf_extractor.py)**

**Tool:** PyMuPDF (fitz library)

**De ce PyMuPDF È™i nu PyPDF2?**
- 10x mai rapid (~500 PDFs/hour vs 50)
- Nativ suportÄƒ imagini (extract, analyze)
- Mai stabil cu PDFs educaÈ›ionale (diagrame)

**Process:**
```python
# 1. Open PDF
pdf = fitz.open("manual.pdf")

# 2. For each page:
for page in pdf:
    # Extract text
    text = page.get_text()

    # Get images
    images = page.get_images()

    # For each image:
    for img_ref in images:
        pixmap = fitz.Pixmap(pdf, img_ref)
        # Analyze: size, aspect ratio, type
        # Store metadata

# 3. Return
result = {
    "text": "...",  # All text combined
    "images": [
        {
            "page": 1,
            "size": 102400,  # bytes
            "priority_score": 0.9,  # High priority (diagram)
            "type": "diagram"
        },
        ...
    ],
    "total_pages": 45,
    "file_size": 2500000  # bytes
}
```

**Performance:** ~100-200ms per PDF (average 2-10 MB)

---

### 2. **OCR PROCESSING (scripts/ocr_processor.py)**

**Tool:** PaddleOCR (paddle-paddle library)

**De ce PaddleOCR?**
- Gratuit (vs Google Cloud Vision = $3-6 per 1000 images)
- CPU-based (funcÈ›ioneazÄƒ pe Kaggle CPU zones fÄƒrÄƒ GPU)
- 80+ limbi (inclusiv romÃ¢nÄƒ)
- Mai bun pentru diagrame educaÈ›ionale vs Tesseract

**Process:**
```python
# 1. Load PaddleOCR model (first run = download ~500MB)
ocr = PaddleOCR(lang=['ro', 'en'])

# 2. For each image (selective based on priority):
if image.priority_score > 0.8:
    result = ocr.ocr(image_array)

    # result = [
    #     [
    #         ([[x1, y1], [x2, y2], ...], ("text", confidence)),
    #         ...
    #     ]
    # ]

# 3. Extract text + confidence
ocr_text = "\n".join([line[1][0] for line in result[0]])

# 4. Merge with PDF text
combined_text = pdf_text + "\n" + ocr_text
```

**Performance:**
- ~10-30 seconds per image (CPU)
- ~2-5 seconds per image (GPU - but we skip GPU for stability)
- Selective: 60% images skipped (low priority)

---

### 3. **TEXT CHUNKING (scripts/chunking.py)**

**De ce chunking?**
- Embeddings au context window (token limit)
- Matching: full PDF = 10,000+ chars â†’ exceeds token limit
- Solution: split Ã®n chunks, each ~500 chars

**Strategy:**
```
Raw text: "Aria pÄƒtrat = laturaÂ²... (1000 chars total) ...perimetru"

Chunking:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 1 (500 chars)              â”‚
â”‚ "Aria pÄƒtrat = laturaÂ²..."       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€ Overlap (50 chars)
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 2 (500 chars)              â”‚
â”‚ "...aturaÂ²... perimetru..."      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Overlap ensures: semantic continuity between chunks
```

**Deduplication:**
- Calculate MD5 hash: hash("text content")
- Skip if hash seen before
- Eliminates: headers, footers, repeated disclaimers

**Output:** List[Chunk]
```python
[
    Chunk(
        text="Aria pÄƒtrat = laturaÂ²...",
        chunk_id="chunk_001",
        chunk_hash="abc123...",
        source_page=1,
        metadata={'char_count': 500, ...}
    ),
    ...
]
```

---

### 4. **EMBEDDING GENERATION (scripts/embedding_generator.py)**

**Model:** `paraphrase-multilingual-mpnet-base-v2`

**Architecture:**
```
Text Input (max 384 tokens)
    â†“
Tokenizer (BERT)
    â†“
Embedding Layer (768 hidden units)
    â†“
Transformer Blocks (Ã—12 layers)
    â†“
Mean Pooling (aggregate all tokens)
    â†“
Output: Vector (768 dimensions)
```

**Why 768 dimensions?**
- 384 dimensions = too small (loses semantic nuances)
- 1024+ dimensions = too large (slow, memory intensive)
- 768 = Goldilocks zone for semantic similarity + efficiency

**Performance:**
- Single text: ~50ms (CPU)
- Batch 128: ~200ms total = 1.5ms/text (GPU batching)
- Rate: ~50,000 vectors/hour (GPU)

**Quality:**
- Multi-lingual: handles Romanian + English seamlessly
- Semantic: "aria pÄƒtrat" â‰ˆ embedding similar to "square area"
- Robust: handles misspellings, synonyms

---

### 5. **BATCH UPLOAD (scripts/supabase_uploader.py)**

**Why batching?**
- 600k individual inserts = 600k API calls = hours
- 60 batches Ã— 10k vectors = 60 API calls = minutes
- 10x faster + less network overhead

**Upload sequence:**
```
Batch 1: vectors 1-10k      â†’ 20-30 seconds
Batch 2: vectors 10k-20k    â†’ 20-30 seconds
...
Batch 60: vectors 590k-600k â†’ 20-30 seconds
---
Total: ~20 minutes (vectori) + 30-60 min (index creation)
```

**Retry logic:**
```python
for attempt in range(3):  # Max 3 attempts
    try:
        upload_batch()
        break  # Success, move to next batch
    except Exception:
        if attempt < 2:
            sleep(5)  # Wait before retry
        else:
            failed_count += 1  # Record failure
```

---

### 6. **INDEX CREATION (Supabase SQL)**

**Why HNSW Index?**
- Without: similarity query = full table scan = 10+ seconds
- With HNSW: ~50-100ms per query (600k vectors)

**HNSW (Hierarchical Navigable Small World):**
- Graph-based nearest neighbor search
- Trade-off: 15% memory overhead for 100x speed
- Better than: linear scan, LSH, or PQ for embeddings

**SQL:**
```sql
CREATE INDEX idx_embedding_hnsw
ON document_embeddings
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

**Parameters:**
- `m = 16`: graph connectivity (higher = more accurate but more memory)
- `ef_construction = 64`: construction parameter (higher = better index)

**Index building time:**
- 100k vectors: ~5 minutes
- 600k vectors: ~30-60 minutes
- One-time cost (done once after upload)

---

## ğŸ“Š Data Flow Example

Let's trace one complete document:

```
INPUT PDF: "clasa_1/matematica/cap_01_adunare.pdf" (3.2 MB, 45 pages)

1. PDF EXTRACTION
   â”œâ”€ Extract pages 1-45: "1 + 1 = 2. 2 + 3 = 5..."
   â”œâ”€ Find 8 images (diagrams with numbers)
   â”œâ”€ Filter 3 images (too small or decorative)
   â””â”€ Output: {text: "4500 chars", images: [5 diagrams]}

2. OCR PROCESSING
   â”œâ”€ Process 5 images (priority > 0.8)
   â”œâ”€ Extract: "Exercise 1: Calculate 5+7"
   â”œâ”€ Merge with PDF text
   â””â”€ Output: {text: "5200 chars" (with OCR)}

3. TEXT CHUNKING
   â”œâ”€ Split at periods: [sent1, sent2, ..., sent50]
   â”œâ”€ Group into chunks of ~500 chars
   â”œâ”€ Add overlaps (50 chars)
   â”œâ”€ Remove duplicates (footer on all pages)
   â””â”€ Output: [Chunk1, Chunk2, ..., Chunk11] (11 chunks)

4. EMBEDDING GENERATION
   â”œâ”€ Batch all chunks: 128 at a time (but only 11 here)
   â”œâ”€ Tokenize: split text into tokens (max 384)
   â”œâ”€ Forward pass: 11 texts â†’ 11 vectors (768-dim each)
   â””â”€ Output: numpy array (11, 768)

5. METADATA ENRICHMENT
   â”œâ”€ chunk_id: "clasa_1_matematica_cap01_chunk_1" ... "chunk_11"
   â”œâ”€ source_pdf: "clasa_1/matematica/cap_01_adunare.pdf"
   â”œâ”€ page_num: 1, 3, 5, 8, ... (page where chunk from)
   â”œâ”€ clasa: 1
   â”œâ”€ materie: "MatematicÄƒ"
   â”œâ”€ capitol: "Capitolul 1 - Adunare"
   â””â”€ has_images: true (because used OCR)

6. SUPABASE UPLOAD
   â”œâ”€ Format: [
   â”‚   {
   â”‚     "chunk_id": "clasa_1_matematica_cap01_chunk_1",
   â”‚     "text": "1 + 1 = 2. 2 + 2 = 4...",
   â”‚     "embedding": "[0.123, 0.456, ..., -0.789]",  // 768 floats
   â”‚     "source_pdf": "clasa_1/matematica/cap_01_adunare.pdf",
   â”‚     "page_num": 1,
   â”‚     "clasa": 1,
   â”‚     "materie": "MatematicÄƒ",
   â”‚     "capitol": "Capitolul 1 - Adunare",
   â”‚     "chunk_hash": "a1b2c3d4e5f6...",  // MD5(text)
   â”‚     "has_images": true
   â”‚   },
   â”‚   ...  // 10 more chunks
   â”‚ ]
   â”‚
   â”œâ”€ Batch insert: 11 rows inserted into DB
   â””â”€ Status: "OK", 11/11 inserted

RESULT in Supabase:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ chunk_id â”‚ text                       â”‚ embedding        â”‚ clasa â”‚ materie       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ..._1    â”‚ "1 + 1 = 2..."             â”‚ [0.123, ...]     â”‚ 1     â”‚ MatematicÄƒ    â”‚
â”‚ ..._2    â”‚ "2 + 3 = 5..."             â”‚ [0.456, ...]     â”‚ 1     â”‚ MatematicÄƒ    â”‚
â”‚ ...      â”‚ ...                        â”‚ ...              â”‚ ...   â”‚ ...           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Query Time (Usage in AI App)

```python
# User: "Cum se calculeazÄƒ suma 5 + 7?"

# 1. Generate query embedding (same model as training)
query_text = "Cum se calculeazÄƒ suma 5 + 7?"
query_embedding = embedding_model.encode([query_text])[0]  # vector(768)

# 2. Query Supabase
results = supabase.rpc('match_documents', {
    'query_embedding': query_embedding.tolist(),
    'match_count': 10,
    'filter_clasa': 1,  # Only class 1
    'filter_materie': 'MatematicÄƒ'
})

# 3. Get results
results = [
    {
        'text': '5 + 7 = 12',
        'similarity': 0.92,  # cosine similarity
        'metadata': {
            'source_pdf': 'clasa_1/matematica.pdf',
            'page_num': 15,
            'capitol': 'Adunare'
        }
    },
    {
        'text': 'Exercise: Calculate 5 + 7 + 3',
        'similarity': 0.85,
        ...
    },
    ...  # up to 10 results
]

# 4. Format for LLM
context = "\n".join([f"- {r['text']}" for r in results])
prompt = f"""
RÄƒspunde la Ã®ntrebarea: "Cum se calculeazÄƒ suma 5 + 7?"
Folosind materialele din manual:
{context}
"""

# 5. Call LLM (GPT-4, Claude, etc)
response = llm.generate(prompt)
# Output: "Suma 5 + 7 = 12. Pentru a calcula:
#         5 + 5 = 10, plus 2 mai = 12."
```

**Latency breakdown:**
- Generate query embedding: ~50ms (CPU)
- Query Supabase: ~100ms (HNSW search)
- Retrieve results: ~50ms (network)
- **Total: ~200ms before LLM**

---

## ğŸ” Database Security

**Current (Free Tier):**
- Anon key: public, safe for SELECT
- Service role key: secret, never in frontend
- RLS: Disabled (public read-only access)

**For Production:**
- Enable RLS (Row Level Security)
- Create policies per user/role
- Use service role key only for backend
- Rotate keys periodically

---

## ğŸ“ˆ Scalability

**Current: 600k vectors Ã— 768 dimensions**

```
Storage calculation:
- Vector: 768 floats Ã— 4 bytes = 3.07 KB
- Metadata: ~500 bytes
- Total per row: ~3.5 KB
- 600k rows Ã— 3.5 KB = ~2.1 GB (but Supabase stores compressed ~300-500 MB)

Query performance:
- HNSW index: ~100ms for top-10 from 600k
- Linear scaling: 10M vectors â†’ ~100-200ms (index scales efficiently)

Limits on Supabase free tier:
- 500 MB storage: supports ~150-170k vectors (uncompressed)
- But actual usage: ~300-500 MB (compressed pgvector)
- Current 600k: close to limit
```

**If you need >600k vectors:**
1. Upgrade Supabase to paid plan ($15/month â†’ 100GB)
2. Or split into multiple projects
3. Or move to managed vector database (Pinecone, Weaviate)

---

## ğŸ› ï¸ Troubleshooting Flow

```
Problem: "Slow queries"
â†’ Check HNSW index exists?
  ```sql
  SELECT * FROM pg_indexes WHERE tablename = 'document_embeddings'
  ```
â†’ If no HNSW index: run CREATE INDEX
â†’ If index exists: check query plan
  ```sql
  EXPLAIN ANALYZE SELECT * FROM match_documents(...)
  ```

Problem: "Out of memory during processing"
â†’ Reduce batch size: embeddings.batch_size = 32
â†’ Reduce chunk overlap: pdf.overlap = 20
â†’ Process PDFs in smaller groups (checkpoint system)

Problem: "Upload fails on specific vectors"
â†’ Check vector dimensions: should be exactly 768
â†’ Check text encoding: UTF-8
â†’ Check for NULL values in required columns
â†’ Verify API key permissions
```

---

## ğŸ“š Next Steps

Once embeddings are in Supabase:

1. **AI Tutoring App:** Use match_documents() for context retrieval
2. **RAG (Retrieval Augmented Generation):** Feed retrieval results to LLM
3. **Search UI:** Allow users to query embeddings directly
4. **Analytics:** Track which topics are searched most
5. **Recommendations:** Suggest similar topics to users

---

**Status: Embeddings Processing Pipeline Ready!**
